{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# MusicGen-Large Finetuning Loop (Sequential Processing) - Transformers Version\n",
        "\n",
        "このノートブックは、大規模なデータセットをZIPファイル単位で順次処理（解凍→学習→削除）しながらMusicGen-Largeをファインチューニングします。\n",
        "Hugging Face Transformersライブラリを使用します。\n",
        "\n",
        "## 前提条件\n",
        "1. Google Driveに以下のデータがあること\n",
        "   - `MyData/Archive_wavs/metadata.jsonl`: 全データのメタデータ\n",
        "   - `MyData/Archive_wavs/archive_batch_xxxx.zip`: 音声データのZIPファイル群\n",
        "2. A100 GPU推奨（VRAM容量のため）\n",
        "3. **WandB API Key**: Colabのシークレット（鍵マーク）に `WANDB_API_KEY` という名前で登録してください。"
      ],
      "id": "header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# @title 1. 環境設定とライブラリインストール\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"潜在的な問題のあるライブラリをアンインストール中...\")\n",
        "# 関連するライブラリを一度アンインストール\n",
        "!pip uninstall -y torch torchvision torchaudio torchcodec fastai sentence-transformers\n",
        "print(\"アンインストール完了。\")\n",
        "\n",
        "print(\"ライブラリをインストール中...\")\n",
        "\n",
        "# CUDA 12.6対応のPyTorch (Nightly or Pre-release)\n",
        "# 注意: ユーザー指定によりCUDA 12.6をターゲットにします。\n",
        "!pip install --pre torch torchvision torchaudio torchcodec --index-url https://download.pytorch.org/whl/nightly/cu126\n",
        "\n",
        "# Hugging Face Libraries & WandB\n",
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U datasets accelerate bitsandbytes wandb\n",
        "\n",
        "# FFmpegのインストール\n",
        "!apt-get update\n",
        "!apt-get install -y ffmpeg\n",
        "print(\"FFmpegのインストール完了。\")\n",
        "\n",
        "print(\"インストール完了。\")"
      ],
      "id": "setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# @title 1.1 ライブラリのインポート\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from google.colab import drive, userdata\n",
        "import wandb\n",
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration, Trainer, TrainingArguments, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Audio\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "print(\"ライブラリのインポート完了。\")"
      ],
      "id": "imports"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wandb_login"
      },
      "outputs": [],
      "source": [
        "# @title 1.5 WandB ログイン\n",
        "try:\n",
        "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "    wandb.login(key=wandb_api_key)\n",
        "    print(\"WandBへのログインに成功しました。\")\n",
        "except Exception as e:\n",
        "    print(f\"WandBへのログインに失敗しました: {e}\")\n",
        "    print(\"Colabのシークレットに 'WANDB_API_KEY' が設定されているか確認してください。\")"
      ],
      "id": "wandb_login"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drive_mount"
      },
      "outputs": [],
      "source": [
        "# @title 2. Google Drive マウント\n",
        "drive.mount('/content/drive')"
      ],
      "id": "drive_mount"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paths_config"
      },
      "outputs": [],
      "source": [
        "# @title 3. パスと設定の定義\n",
        "# --- ユーザー設定エリア ---\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
        "DATA_ROOT = DRIVE_ROOT / 'Archive_Wavs'\n",
        "METADATA_PATH = DATA_ROOT / 'metadata.jsonl'\n",
        "ZIP_DIR = DATA_ROOT\n",
        "\n",
        "# 出力先（チェックポイント保存場所）\n",
        "OUTPUT_DIR = DRIVE_ROOT / 'MusicGen_Finetuning_Output'\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# 一時作業ディレクトリ（Colabローカル）\n",
        "TEMP_WORK_DIR = Path('/content/temp_work')\n",
        "TEMP_DATA_DIR = TEMP_WORK_DIR / 'data'\n",
        "\n",
        "TEMP_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(f\"Metadata: {METADATA_PATH}\")\n",
        "print(f\"Zip Dir: {ZIP_DIR}\")\n",
        "print(f\"Output Dir: {OUTPUT_DIR}\")"
      ],
      "id": "paths_config"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper_functions"
      },
      "outputs": [],
      "source": [
        "# @title 4. ヘルパー関数の定義\n",
        "def extract_zip(zip_path, extract_to):\n",
        "    \"\"\"ZIPファイルを指定ディレクトリに解凍する\"\"\"\n",
        "    print(f\"{zip_path} を {extract_to} に解凍中...\")\n",
        "    if extract_to.exists():\n",
        "        shutil.rmtree(extract_to)\n",
        "    extract_to.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    subprocess.run(['unzip', '-q', str(zip_path), '-d', str(extract_to)], check=True)\n",
        "    print(\"解凍完了。\")\n",
        "\n",
        "def create_batch_metadata(main_metadata_path, current_wav_dir, output_jsonl_path):\n",
        "    \"\"\"\n",
        "    メインのmetadata.jsonlから、現在解凍されているファイルに対応するエントリのみを抽出し、\n",
        "    パスをColab上の絶対パスに書き換えて新しいjsonlを作成する。\n",
        "    \"\"\"\n",
        "    print(f\"{output_jsonl_path} にバッチメタデータを作成中...\")\n",
        "\n",
        "    extracted_files = list(current_wav_dir.rglob('*.wav'))\n",
        "    extracted_files_map = {f.name: f for f in extracted_files}\n",
        "\n",
        "    valid_entries = []\n",
        "\n",
        "    with open(main_metadata_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                orig_path = entry.get('path', '')\n",
        "                filename = os.path.basename(orig_path)\n",
        "\n",
        "                if filename in extracted_files_map:\n",
        "                    # パスを絶対パスに更新\n",
        "                    entry['path'] = str(extracted_files_map[filename])\n",
        "                    # TransformersのDatasetで読み込むために 'audio' キーにパスを入れるのが一般的だが\n",
        "                    # ここでは後処理でロードするため 'path' のままでもOK。\n",
        "                    # ただし、datasets libraryのAudio機能を使うなら 'audio': path が便利。\n",
        "                    entry['audio'] = str(extracted_files_map[filename])\n",
        "                    valid_entries.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "    if not valid_entries:\n",
        "        print(\"警告: 解凍されたファイルに一致するメタデータが見つかりませんでした。\")\n",
        "        return False\n",
        "\n",
        "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
        "        for entry in valid_entries:\n",
        "            f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "    print(f\"{len(valid_entries)} 件のエントリを持つメタデータを作成しました。\")\n",
        "    return True\n",
        "\n",
        "def preprocess_function(examples, processor, model, audio_column_name=\"audio\", text_column_name=\"caption\"):\n",
        "    \"\"\"データセットの前処理関数\"\"\"\n",
        "    print(\"------ preprocess_function 開始 ------\")\n",
        "\n",
        "    processed_audio_arrays = []\n",
        "    valid_indices = []\n",
        "    # サンプリングレートを初期化（バッチ内の全サンプルで同一と仮定）\n",
        "    # 最初の有効なサンプルから取得するのが安全\n",
        "    sampling_rate = None\n",
        "\n",
        "    # 音声配列のフィルタリングと処理\n",
        "    for idx, audio_info in enumerate(examples[audio_column_name]):\n",
        "        print(f\"[Debug preprocess] Processing audio sample {idx}. Audio info: {audio_info}\")\n",
        "        audio_array = audio_info[\"array\"]\n",
        "        if audio_array is not None and len(audio_array) > 0:\n",
        "            if sampling_rate is None:\n",
        "                sampling_rate = audio_info[\"sampling_rate\"]\n",
        "                print(f\"[Debug preprocess] Detected sampling rate: {sampling_rate}\")\n",
        "\n",
        "            # 必要に応じて2D（ステレオ）配列をチャンネル平均で1D（モノラル）に変換\n",
        "            if audio_array.ndim == 2:\n",
        "                # フォーマットは (channels, samples) または (samples, channels) と仮定\n",
        "                # どちらの軸が小さい次元か（おそらくチャンネル）を確認\n",
        "                if audio_array.shape[0] < audio_array.shape[1]: # (channels, samples)\n",
        "                    audio_array = np.mean(audio_array, axis=0)\n",
        "                else: # (samples, channels)\n",
        "                    audio_array = np.mean(audio_array, axis=1)\n",
        "                print(f\"[Debug preprocess] Converted 2D audio to 1D. New shape: {audio_array.shape}\")\n",
        "\n",
        "            # float32型の1D NumPy配列であることを確認\n",
        "            if audio_array.ndim == 1:\n",
        "                processed_audio_arrays.append(audio_array.astype(np.float32))\n",
        "                valid_indices.append(idx)\n",
        "                print(f\"[Debug preprocess] Added valid audio sample {idx}. Current processed_audio_arrays count: {len(processed_audio_arrays)}\")\n",
        "            else:\n",
        "                print(f\"バッチ内のインデックス {idx} の問題のある音声サンプルをスキップ: 変換後に1Dではありません (ndim={audio_array.ndim})。\")\n",
        "\n",
        "        else:\n",
        "            print(f\"バッチ内のインデックス {idx} の問題のある音声サンプルをスキップ: 空またはNoneです。\")\n",
        "\n",
        "    if not processed_audio_arrays:\n",
        "        print(\"警告: 処理可能な音声データがこのバッチにはありません。空の入力を返します。\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"[Debug preprocess] Number of processed audio arrays: {len(processed_audio_arrays)}\")\n",
        "\n",
        "    # 有効なインデックスに基づいてテキストをフィルタリング\n",
        "    texts = [examples.get(text_column_name, [\"\"] * len(examples[audio_column_name]))[i] for i in valid_indices]\n",
        "    # プライマリキーが空または欠落している場合、他のテキストキーを処理\n",
        "    for i, text_val in enumerate(texts):\n",
        "        if not text_val:\n",
        "            if \"text\" in examples and valid_indices[i] < len(examples[\"text\"]):\n",
        "                texts[i] = examples[\"text\"][valid_indices[i]]\n",
        "            elif \"description\" in examples and valid_indices[i] < len(examples[\"description\"]):\n",
        "                texts[i] = examples[\"description\"][valid_indices[i]]\n",
        "    # すべてのテキストが文字列であることを確認\n",
        "    texts = [str(t) if t else \"\" for t in texts]\n",
        "    print(f\"[Debug preprocess] Texts for batch: {texts[:5]}...\") # Print first 5 texts\n",
        "\n",
        "    # 音声とテキストの最大長を定義\n",
        "    # MusicGenは通常約30秒の音声を処理 (32kHz * 30s = 960,000 samples)\n",
        "    # Encodecの一般的な最大長を使用 (約7.68秒 = 245760 samples)\n",
        "    MAX_AUDIO_SAMPLES = 245760 # Approx 7.68 seconds at 32kHz\n",
        "    MAX_TEXT_TOKENS = 256\n",
        "\n",
        "    # 特徴抽出器を使用して音声入力を処理\n",
        "    audio_features = processor.feature_extractor(\n",
        "        processed_audio_arrays, # Use cleaned audio arrays\n",
        "        sampling_rate=sampling_rate,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_AUDIO_SAMPLES,\n",
        "    )\n",
        "    print(f\"[Debug preprocess] audio_features.input_values type: {type(audio_features.input_values)}\")\n",
        "\n",
        "    # テンソル変換前に input_values と attention_mask のNumPy配列の一貫性を手動で確認\n",
        "    input_values_to_normalize = audio_features.input_values\n",
        "    if not isinstance(input_values_to_normalize, list):\n",
        "        input_values_to_normalize = [input_values_to_normalize]\n",
        "\n",
        "    normalized_input_values = []\n",
        "    for audio_arr in input_values_to_normalize:\n",
        "        if not isinstance(audio_arr, np.ndarray):\n",
        "            print(f\"input_values 内の不正な形式の audio_arr をスキップ: type={type(audio_arr)}\")\n",
        "            continue\n",
        "\n",
        "        if audio_arr.ndim == 2:\n",
        "            if audio_arr.shape[0] < audio_arr.shape[1]:\n",
        "                audio_arr = np.mean(audio_arr, axis=0)\n",
        "            else:\n",
        "                audio_arr = np.mean(audio_arr, axis=1)\n",
        "\n",
        "        if audio_arr.ndim != 1:\n",
        "            print(f\"feature_extractor からの問題のある音声サンプルをスキップ: 変換後に1Dではありません (ndim={audio_arr.ndim})。\")\n",
        "            continue\n",
        "\n",
        "        current_length = audio_arr.shape[-1]\n",
        "        if current_length > MAX_AUDIO_SAMPLES:\n",
        "            normalized_input_values.append(audio_arr[:MAX_AUDIO_SAMPLES])\n",
        "        elif current_length < MAX_AUDIO_SAMPLES:\n",
        "            normalized_input_values.append(np.pad(audio_arr, (0, MAX_AUDIO_SAMPLES - current_length), 'constant', constant_values=0))\n",
        "        else:\n",
        "            normalized_input_values.append(audio_arr)\n",
        "\n",
        "    if not normalized_input_values:\n",
        "        print(\"警告: 処理可能な正規化された音声入力がこのバッチにはありません。空の入力を返します。\")\n",
        "        return {}\n",
        "\n",
        "    input_values_np = np.stack(normalized_input_values).astype(np.float32)\n",
        "    input_values_np = input_values_np[:, np.newaxis, :]\n",
        "\n",
        "    print(f\"[Debug preprocess] input_values_np final shape: {input_values_np.shape}, dtype: {input_values_np.dtype}\")\n",
        "\n",
        "    audio_inputs_tensor = {\n",
        "        \"input_values\": torch.from_numpy(input_values_np),\n",
        "    }\n",
        "\n",
        "    if \"attention_mask\" in audio_features and audio_features.attention_mask is not None:\n",
        "        attention_mask_to_normalize = audio_features.attention_mask\n",
        "        if not isinstance(attention_mask_to_normalize, list):\n",
        "            attention_mask_to_normalize = [attention_mask_to_normalize]\n",
        "\n",
        "        normalized_attention_mask = []\n",
        "        for mask_arr in attention_mask_to_normalize:\n",
        "            if not isinstance(mask_arr, np.ndarray):\n",
        "                print(f\"attention_mask 内の不正な形式の mask_arr をスキップ: type={type(mask_arr)}\")\n",
        "                continue\n",
        "\n",
        "            if mask_arr.ndim == 2:\n",
        "                if mask_arr.shape[0] < mask_arr.shape[1]:\n",
        "                    mask_arr = np.mean(mask_arr, axis=0).astype(np.longlong)\n",
        "                else:\n",
        "                    mask_arr = np.mean(mask_arr, axis=1).astype(np.longlong)\n",
        "\n",
        "            if mask_arr.ndim != 1:\n",
        "                print(f\"feature_extractor からの問題のある mask_arr をスキップ: 変換後に1Dではありません (ndim={mask_arr.ndim})。\")\n",
        "                continue\n",
        "\n",
        "            current_length = mask_arr.shape[-1]\n",
        "            if current_length > MAX_AUDIO_SAMPLES:\n",
        "                normalized_attention_mask.append(mask_arr[:MAX_AUDIO_SAMPLES])\n",
        "            elif current_length < MAX_AUDIO_SAMPLES:\n",
        "                normalized_attention_mask.append(np.pad(mask_arr, (0, MAX_AUDIO_SAMPLES - current_length), 'constant', constant_values=0))\n",
        "            else:\n",
        "                normalized_attention_mask.append(mask_arr)\n",
        "\n",
        "        if not normalized_attention_mask:\n",
        "            audio_inputs_tensor[\"attention_mask\"] = torch.ones(\n",
        "                audio_inputs_tensor[\"input_values\"].shape[0],\n",
        "                audio_inputs_tensor[\"input_values\"].shape[2],\n",
        "                dtype=torch.long\n",
        "            )\n",
        "        else:\n",
        "            attention_mask_np = np.stack(normalized_attention_mask).astype(np.longlong)\n",
        "            audio_inputs_tensor[\"attention_mask\"] = torch.from_numpy(attention_mask_np)\n",
        "    else:\n",
        "        audio_inputs_tensor[\"attention_mask\"] = torch.ones(\n",
        "            audio_inputs_tensor[\"input_values\"].shape[0],\n",
        "            audio_inputs_tensor[\"input_values\"].shape[2],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "    print(f\"[Debug preprocess] audio_inputs_tensor['attention_mask'] shape: {audio_inputs_tensor['attention_mask'].shape}, dtype: {audio_inputs_tensor['attention_mask'].dtype}\")\n",
        "\n",
        "    text_inputs = processor.tokenizer(\n",
        "        text=texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_TEXT_TOKENS,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    print(f\"[Debug preprocess] text_inputs.input_ids shape: {text_inputs.input_ids.shape}, dtype: {text_inputs.input_ids.dtype}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_values_for_encode = audio_inputs_tensor[\"input_values\"].to(model.device)\n",
        "        padding_mask_for_encode = audio_inputs_tensor[\"attention_mask\"].to(model.device)\n",
        "\n",
        "        print(f\"[Debug preprocess] Before audio_encoder.encode - input_values_for_encode shape: {input_values_for_encode.shape}, dtype: {input_values_for_encode.dtype}, device: {input_values_for_encode.device}\")\n",
        "        print(f\"[Debug preprocess] Before audio_encoder.encode - padding_mask_for_encode shape: {padding_mask_for_encode.shape}, dtype: {padding_mask_for_encode.dtype}, device: {padding_mask_for_encode.device}\")\n",
        "\n",
        "        if not torch.isfinite(input_values_for_encode).all():\n",
        "            print(f\"警告: input_values_for_encode に非有限値 (NaN/Inf) が含まれています。バッチをスキップします。\")\n",
        "            return {}\n",
        "\n",
        "        audio_codes = None\n",
        "        try:\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                audio_codes, audio_scales, _ = model.audio_encoder.encode(\n",
        "                    input_values_for_encode,\n",
        "                    padding_mask_for_encode,\n",
        "                    model.audio_encoder.config.target_bandwidths[-1]\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"エラー: model.audio_encoder.encode 中に例外が発生しました: {e}\")\n",
        "            print(\"------ preprocess_function 終了 (エラー) ------\")\n",
        "            return {}\n",
        "\n",
        "        if not isinstance(audio_codes, torch.Tensor):\n",
        "            print(f\"エラー: model.audio_encoder.encode が予期しない非テンソル型の audio_codes を返しました: {type(audio_codes)}\")\n",
        "            print(f\"詳細: audio_codes の値は '{str(audio_codes)}' でした。\")\n",
        "            print(\"警告: オーディオエンコーダーからの予期せぬ出力のため、このバッチのデータ処理をスキップします。\")\n",
        "            print(\"------ preprocess_function 終了 (エラー) ------\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"[Debug preprocess] After audio_encoder.encode - audio_codes shape: {audio_codes.shape}, dtype: {audio_codes.dtype}\")\n",
        "\n",
        "        if audio_codes.shape[-1] > 1:\n",
        "            labels_audio_codes = audio_codes[:, 1:]\n",
        "            decoder_input_ids_audio_codes = audio_codes[:, :-1]\n",
        "        else:\n",
        "            print(\"警告: オーディオコードが短すぎてシフトできません。labels と decoder_input_ids を同じ値に設定します。\")\n",
        "            labels_audio_codes = audio_codes\n",
        "            decoder_input_ids_audio_codes = audio_codes\n",
        "\n",
        "        labels_audio_codes = labels_audio_codes.cpu()\n",
        "        decoder_input_ids_audio_codes = decoder_input_ids_audio_codes.cpu()\n",
        "\n",
        "    inputs = {\n",
        "        \"input_values\": audio_inputs_tensor[\"input_values\"],\n",
        "        \"padding_mask\": audio_inputs_tensor[\"attention_mask\"],\n",
        "        \"input_ids\": text_inputs.input_ids,\n",
        "        \"attention_mask\": text_inputs.attention_mask,\n",
        "        \"decoder_input_ids\": decoder_input_ids_audio_codes,\n",
        "        \"labels\": labels_audio_codes,\n",
        "    }\n",
        "    print(\"------ preprocess_function 終了 (成功) ------\")\n",
        "    return inputs"
      ],
      "id": "helper_functions"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_loop"
      },
      "outputs": [],
      "source": [
        "# @title 5. メインループ実行\n",
        "import torch # Added import statement for torch\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig, MusicgenForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model # Added LoraConfig and get_peft_model imports\n",
        "\n",
        "# モデルとプロセッサの準備\n",
        "MODEL_ID = \"facebook/musicgen-large\"\n",
        "print(f\"モデルをロード中: {MODEL_ID}...\")\n",
        "\n",
        "# 8-bit 量子化設定\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16 # A100/V100ならfloat16を推奨\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config, # 8-bit 量子化を適用\n",
        "    device_map=\"auto\" # GPUで実行するように変更\n",
        ")\n",
        "\n",
        "# PEFT (LoRA) の設定\n",
        "# MusicGenのエンコーダ (T5EncoderModel) とデコーダ (MusicgenForCausalLM) の両方にLoRAを適用\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRAのランク\n",
        "    lora_alpha=32, # LoRAスケーリング係数\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # LoRAを適用するモジュール (Attention層のQuery, Value)\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\", # MusicGenはSequence-to-Sequenceモデル\n",
        ")\n",
        "\n",
        "# PEFTモデルをオリジナルモデルにアタッチ\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters() # 学習可能なパラメータ数を確認\n",
        "\n",
        "model.train()\n",
        "\n",
        "# ZIPファイルリスト取得\n",
        "zip_files = sorted(list(ZIP_DIR.glob('archive_batch_*.zip')))\n",
        "print(f\"{len(zip_files)} 個のZIPファイルが見つかりました。\")\n",
        "\n",
        "# 以前のチェックポイントがあればロード（簡易実装）\n",
        "latest_checkpoint_path = OUTPUT_DIR / 'latest_checkpoint'\n",
        "if latest_checkpoint_path.exists():\n",
        "    print(f\"{latest_checkpoint_path} から再開します...\")\n",
        "    # PEFTモデルとしてロードする場合\n",
        "    model = MusicgenForConditionalGeneration.from_pretrained(\n",
        "        latest_checkpoint_path,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" # GPUで実行するように変更\n",
        "    )\n",
        "    # LoRAアダプターをロード\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "# GPU設定 (device_map=\"auto\"を使用しているため、model.to(device)は不要)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# CUDAキャッシュをクリアしてメモリを解放\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for i, zip_file in enumerate(zip_files):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"バッチ {i+1}/{len(zip_files)} を処理中: {zip_file.name}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # 1. 解凍\n",
        "    extract_zip(zip_file, TEMP_DATA_DIR)\n",
        "\n",
        "    # 2. メタデータ作成\n",
        "    batch_metadata_path = TEMP_WORK_DIR / 'batch.jsonl'\n",
        "    success = create_batch_metadata(METADATA_PATH, TEMP_DATA_DIR, batch_metadata_path)\n",
        "\n",
        "    if not success:\n",
        "        print(\"メタデータエラーのため、このバッチをスキップします。\")\n",
        "        continue\n",
        "\n",
        "    # 3. データセット準備\n",
        "    dataset = load_dataset(\"json\", data_files=str(batch_metadata_path), split=\"train\")\n",
        "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n",
        "\n",
        "    # 前処理の適用\n",
        "    print(\"データセットを前処理中...\")\n",
        "    encoded_dataset = dataset.map(\n",
        "        lambda x: preprocess_function(x, processor, model),\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names,\n",
        "        batch_size=4 # メモリに応じて調整\n",
        "    )\n",
        "\n",
        "    # 4. トレーニング設定\n",
        "    # バッチごとにTrainerを作り直すが、modelは同じオブジェクトを使い回すことで学習を継続する\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(TEMP_WORK_DIR / \"results\"),\n",
        "        per_device_train_batch_size=2, # A100ならもう少し増やせるかも\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=1e-5,\n",
        "        num_train_epochs=5, # 1バッチあたりのエポック数\n",
        "        save_steps=1000, # バッチ内での保存頻度（必要なら）\n",
        "        logging_steps=10,\n",
        "        fp16=True, # GPU実行のためTrueに設定\n",
        "        save_total_limit=1,\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=\"wandb\", # WandB有効化\n",
        "        run_name=f\"musicgen-finetuning-batch-{i+1}\", # バッチごとにRun名を分ける\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=encoded_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"このバッチのトレーニングを開始します...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # 5. モデル保存\n",
        "    # バッチ完了ごとにDriveへ保存\n",
        "    save_path = OUTPUT_DIR / f'checkpoint_batch_{i+1}'\n",
        "    print(f\"モデルを {save_path} に保存中...\")\n",
        "    model.save_pretrained(save_path)\n",
        "    processor.save_pretrained(save_path)\n",
        "\n",
        "    # 最新版として上書き\n",
        "    latest_path = OUTPUT_DIR / 'latest_checkpoint'\n",
        "    model.save_pretrained(latest_path)\n",
        "    processor.save_pretrained(latest_path)\n",
        "\n",
        "    # 6. クリーンアップ\n",
        "    print(\"一時データをクリーンアップ中...\")\n",
        "    shutil.rmtree(TEMP_DATA_DIR)\n",
        "    TEMP_DATA_DIR.mkdir(exist_ok=True)\n",
        "    # Trainerのクリーンアップ（メモリ解放のため）\n",
        "    del trainer\n",
        "    del dataset\n",
        "    del encoded_dataset\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"すべてのバッチが処理されました。\")"
      ],
      "id": "main_loop"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}