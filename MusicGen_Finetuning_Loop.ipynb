{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# MusicGen-Large Finetuning Loop (Sequential Processing) - Transformers Version\n",
    "\n",
    "このノートブックは、大規模なデータセットをZIPファイル単位で順次処理（解凍→学習→削除）しながらMusicGen-Largeをファインチューニングします。\n",
    "Hugging Face Transformersライブラリを使用します。\n",
    "\n",
    "## 前提条件\n",
    "1. Google Driveに以下のデータがあること\n",
    "   - `MyData/Archive_wavs/metadata.jsonl`: 全データのメタデータ\n",
    "   - `MyData/Archive_wavs/archive_batch_xxxx.zip`: 音声データのZIPファイル群\n",
    "2. A100 GPU推奨（VRAM容量のため）\n",
    "3. **WandB API Key**: Colabのシークレット（鍵マーク）に `WANDB_API_KEY` という名前で登録してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# @title 1. 環境設定とライブラリインストール\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Uninstalling potentially problematic libraries...\")\n",
    "# 関連するライブラリを一度アンインストール\n",
    "!pip uninstall -y torch torchvision torchaudio torchcodec\n",
    "print(\"Uninstall complete.\")\n",
    "\n",
    "print(\"Installing libraries...\")\n",
    "\n",
    "# CUDA 12.6対応のPyTorch (Nightly or Pre-release)\n",
    "# 注意: ユーザー指定によりCUDA 12.6をターゲットにします。\n",
    "!pip install --pre torch torchvision torchaudio torchcodec --index-url https://download.pytorch.org/whl/nightly/cu126\n",
    "\n",
    "# Hugging Face Libraries & WandB\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -U datasets accelerate bitsandbytes wandb\n",
    "\n",
    "# FFmpegのインストール\n",
    "!apt-get update\n",
    "!apt-get install -y ffmpeg\n",
    "print(\"FFmpeg installation complete.\")\n",
    "\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# @title 1.1 ライブラリのインポート\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from google.colab import drive, userdata\n",
    "import wandb\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Audio\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wandb_login"
   },
   "outputs": [],
   "source": [
    "# @title 1.5 WandB ログイン\n",
    "try:\n",
    "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"Logged in to WandB successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB login failed: {e}\")\n",
    "    print(\"Please ensure 'WANDB_API_KEY' is set in Colab secrets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_mount"
   },
   "outputs": [],
   "source": [
    "# @title 2. Google Drive マウント\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paths_config"
   },
   "outputs": [],
   "source": [
    "# @title 3. パスと設定の定義\n",
    "# --- ユーザー設定エリア ---\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
    "DATA_ROOT = DRIVE_ROOT / 'Archive_Wavs'\n",
    "METADATA_PATH = DATA_ROOT / 'metadata.jsonl'\n",
    "ZIP_DIR = DATA_ROOT\n",
    "\n",
    "# 出力先（チェックポイント保存場所）\n",
    "OUTPUT_DIR = DRIVE_ROOT / 'MusicGen_Finetuning_Output'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 一時作業ディレクトリ（Colabローカル）\n",
    "TEMP_WORK_DIR = Path('/content/temp_work')\n",
    "TEMP_DATA_DIR = TEMP_WORK_DIR / 'data'\n",
    "\n",
    "TEMP_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Metadata: {METADATA_PATH}\")\n",
    "print(f\"Zip Dir: {ZIP_DIR}\")\n",
    "print(f\"Output Dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_functions"
   },
   "outputs": [],
   "source": [
    "# @title 4. ヘルパー関数の定義\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"ZIPファイルを指定ディレクトリに解凍する\"\"\"\n",
    "    print(f\"Extracting {zip_path} to {extract_to}...\")\n",
    "    if extract_to.exists():\n",
    "        shutil.rmtree(extract_to)\n",
    "    extract_to.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    subprocess.run(['unzip', '-q', str(zip_path), '-d', str(extract_to)], check=True)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "def create_batch_metadata(main_metadata_path, current_wav_dir, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    メインのmetadata.jsonlから、現在解凍されているファイルに対応するエントリのみを抽出し、\n",
    "    パスをColab上の絶対パスに書き換えて新しいjsonlを作成する。\n",
    "    \"\"\"\n",
    "    print(f\"Creating batch metadata at {output_jsonl_path}...\")\n",
    "\n",
    "    extracted_files = list(current_wav_dir.rglob('*.wav'))\n",
    "    extracted_files_map = {f.name: f for f in extracted_files}\n",
    "\n",
    "    valid_entries = []\n",
    "\n",
    "    with open(main_metadata_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                orig_path = entry.get('path', '')\n",
    "                filename = os.path.basename(orig_path)\n",
    "\n",
    "                if filename in extracted_files_map:\n",
    "                    # パスを絶対パスに更新\n",
    "                    entry['path'] = str(extracted_files_map[filename])\n",
    "                    # TransformersのDatasetで読み込むために 'audio' キーにパスを入れるのが一般的だが\n",
    "                    # ここでは後処理でロードするため 'path' のままでもOK。\n",
    "                    # ただし、datasets libraryのAudio機能を使うなら 'audio': path が便利。\n",
    "                    entry['audio'] = str(extracted_files_map[filename])\n",
    "                    valid_entries.append(entry)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    if not valid_entries:\n",
    "        print(\"Warning: No matching metadata found for extracted files.\")\n",
    "        return False\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in valid_entries:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    print(f\"Created metadata with {len(valid_entries)} entries.\")\n",
    "    return True\n",
    "\n",
    "def preprocess_function(examples, processor, model, audio_column_name=\"audio\", text_column_name=\"caption\"):\n",
    "    \"\"\"データセットの前処理関数\"\"\"\n",
    "    processed_audio_arrays = []\n",
    "    valid_indices = []\n",
    "    # Initialize sampling_rate, assuming all samples in a batch have the same rate.\n",
    "    # It's safer to get it from the first valid sample.\n",
    "    sampling_rate = None\n",
    "\n",
    "    # Filter and process audio arrays\n",
    "    for idx, audio_info in enumerate(examples[audio_column_name]):\n",
    "        audio_array = audio_info[\"array\"]\n",
    "        if audio_array is not None and len(audio_array) > 0:\n",
    "            if sampling_rate is None:\n",
    "                sampling_rate = audio_info[\"sampling_rate\"]\n",
    "\n",
    "            # Convert 2D (stereo) arrays to 1D (mono) by averaging channels if necessary\n",
    "            if audio_array.ndim == 2:\n",
    "                # Assuming format is (channels, samples) or (samples, channels)\n",
    "                # Take mean across the channel axis to convert to mono\n",
    "                # Check which axis has the smaller dimension (likely channels)\n",
    "                if audio_array.shape[0] < audio_array.shape[1]: # (channels, samples)\n",
    "                    audio_array = np.mean(audio_array, axis=0)\n",
    "                else: # (samples, channels)\n",
    "                    audio_array = np.mean(audio_array, axis=1)\n",
    "\n",
    "            # Ensure it's a numpy array of float32 and 1D\n",
    "            if audio_array.ndim == 1:\n",
    "                processed_audio_arrays.append(audio_array.astype(np.float32))\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                print(f\"Skipping problematic audio sample at index {idx} in batch: Not 1D after conversion attempt.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping problematic audio sample at index {idx} in batch: Empty or None.\")\n",
    "\n",
    "    if not processed_audio_arrays:\n",
    "        # If all audio samples in the batch were problematic, return empty inputs\n",
    "        # This will be filtered by the dataset.map later by default if remove_columns=False is not set\n",
    "        # or lead to an empty batch for the model.\n",
    "        return {}\n",
    "\n",
    "    # Filter texts based on valid_indices\n",
    "    texts = [examples.get(text_column_name, [\"\"] * len(examples[audio_column_name]))[i] for i in valid_indices]\n",
    "    # Handle other text key if primary is empty or missing\n",
    "    for i, text_val in enumerate(texts):\n",
    "        if not text_val:\n",
    "            if \"text\" in examples and valid_indices[i] < len(examples[\"text\"]):\n",
    "                texts[i] = examples[\"text\"][valid_indices[i]]\n",
    "            elif \"description\" in examples and valid_indices[i] < len(examples[\"description\"]):\n",
    "                texts[i] = examples[\"description\"][valid_indices[i]]\n",
    "    # Ensure all texts are strings\n",
    "    texts = [str(t) if t else \"\" for t in texts]\n",
    "\n",
    "\n",
    "    # Define max lengths for audio and text\n",
    "    # Musicgen typically processes audio up to ~30 seconds (32kHz * 30s = 960,000 samples)\n",
    "    # Using a common max_length for Encodec (e.68 seconds = 245760 samples)\n",
    "    MAX_AUDIO_SAMPLES = 245760 # Approx 7.68 seconds at 32kHz\n",
    "    MAX_TEXT_TOKENS = 256\n",
    "\n",
    "    # Process audio inputs using the feature extractor\n",
    "    # Do not return_tensors=\"pt\" here, get raw numpy arrays (or lists of arrays) first\n",
    "    audio_features = processor.feature_extractor(\n",
    "        processed_audio_arrays, # Use cleaned audio arrays\n",
    "        sampling_rate=sampling_rate,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_AUDIO_SAMPLES,\n",
    "        # truncation=True, # No longer needed, padding='max_length' handles this for feature_extractor\n",
    "        # return_tensors=\"pt\", # REMOVED: we will convert manually\n",
    "    )\n",
    "\n",
    "    # Manually ensure consistent numpy array for input_values and attention_mask before converting to tensor\n",
    "    # audio_features.input_values might be a list of np.arrays, if so, stack them\n",
    "    # Ensure they are numerical dtype, not object\n",
    "\n",
    "    # Ensure audio_features.input_values is always a list for robust iteration\n",
    "    input_values_to_normalize = audio_features.input_values\n",
    "    if not isinstance(input_values_to_normalize, list):\n",
    "        input_values_to_normalize = [input_values_to_normalize]\n",
    "\n",
    "    # Ensure all audio inputs are exactly MAX_AUDIO_SAMPLES long BEFORE stacking\n",
    "    normalized_input_values = []\n",
    "    for audio_arr in input_values_to_normalize:\n",
    "        # Robustly handle potentially malformed audio_arr from feature_extractor\n",
    "        if not isinstance(audio_arr, np.ndarray):\n",
    "            print(f\"Skipping malformed audio_arr in input_values: type={type(audio_arr)}\")\n",
    "            continue\n",
    "\n",
    "        # Force to 1D if it's 2D (e.g., from feature_extractor itself producing 2D output)\n",
    "        if audio_arr.ndim == 2:\n",
    "            # Assuming format is (channels, samples) or (samples, channels)\n",
    "            # Take mean across the channel axis to convert to mono\n",
    "            if audio_arr.shape[0] < audio_arr.shape[1]: # (channels, samples)\n",
    "                audio_arr = np.mean(audio_arr, axis=0)\n",
    "            else: # (samples, channels)\n",
    "                audio_arr = np.mean(audio_arr, axis=1)\n",
    "\n",
    "        if audio_arr.ndim != 1:\n",
    "            print(f\"Skipping problematic audio sample from feature_extractor: Not 1D after conversion attempt (ndim={audio_arr.ndim}).\")\n",
    "            continue\n",
    "\n",
    "        current_length = audio_arr.shape[-1]\n",
    "        if current_length > MAX_AUDIO_SAMPLES:\n",
    "            normalized_input_values.append(audio_arr[:MAX_AUDIO_SAMPLES])\n",
    "        elif current_length < MAX_AUDIO_SAMPLES:\n",
    "            normalized_input_values.append(np.pad(audio_arr, (0, MAX_AUDIO_SAMPLES - current_length), 'constant', constant_values=0))\n",
    "        else:\n",
    "            normalized_input_values.append(audio_arr)\n",
    "\n",
    "    if not normalized_input_values: # If all were skipped or originally empty, return empty\n",
    "        return {}\n",
    "\n",
    "    input_values_np = np.stack(normalized_input_values).astype(np.float32)\n",
    "\n",
    "    # Reshape to (batch_size, 1, sequence_length) for mono audio\n",
    "    input_values_np = input_values_np[:, np.newaxis, :]\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    audio_inputs_tensor = {\n",
    "        \"input_values\": torch.from_numpy(input_values_np),\n",
    "    }\n",
    "\n",
    "    # Handle attention_mask similarly if it exists or create it\n",
    "    if \"attention_mask\" in audio_features and audio_features.attention_mask is not None:\n",
    "        # Ensure audio_features.attention_mask is always a list for robust iteration\n",
    "        attention_mask_to_normalize = audio_features.attention_mask\n",
    "        if not isinstance(attention_mask_to_normalize, list):\n",
    "            attention_mask_to_normalize = [attention_mask_to_normalize]\n",
    "\n",
    "        normalized_attention_mask = []\n",
    "        for mask_arr in attention_mask_to_normalize:\n",
    "            # Robustly handle potentially malformed mask_arr\n",
    "            if not isinstance(mask_arr, np.ndarray):\n",
    "                print(f\"Skipping malformed mask_arr in attention_mask: type={type(mask_arr)}\")\n",
    "                continue\n",
    "\n",
    "            # Force to 1D if it's 2D\n",
    "            if mask_arr.ndim == 2:\n",
    "                # Assuming format is (channels, samples) or (samples, channels)\n",
    "                if mask_arr.shape[0] < mask_arr.shape[1]: # (channels, samples)\n",
    "                    mask_arr = np.mean(mask_arr, axis=0).astype(np.longlong) # Ensure type after mean\n",
    "                else: # (samples, channels)\n",
    "                    mask_arr = np.mean(mask_arr, axis=1).astype(np.longlong)\n",
    "\n",
    "            if mask_arr.ndim != 1:\n",
    "                print(f\"Skipping problematic mask_arr from feature_extractor: Not 1D after conversion attempt (ndim={mask_arr.ndim}).\")\n",
    "                continue\n",
    "\n",
    "            current_length = mask_arr.shape[-1]\n",
    "            if current_length > MAX_AUDIO_SAMPLES:\n",
    "                normalized_attention_mask.append(mask_arr[:MAX_AUDIO_SAMPLES])\n",
    "            elif current_length < MAX_AUDIO_SAMPLES:\n",
    "                normalized_attention_mask.append(np.pad(mask_arr, (0, MAX_AUDIO_SAMPLES - current_length), 'constant', constant_values=0))\n",
    "            else:\n",
    "                normalized_attention_mask.append(mask_arr)\n",
    "\n",
    "        if not normalized_attention_mask: # If all were skipped or originally empty, fallback to default\n",
    "            # Fallback for empty normalized_attention_mask\n",
    "            audio_inputs_tensor[\"attention_mask\"] = torch.ones(\n",
    "                audio_inputs_tensor[\"input_values\"].shape[0],\n",
    "                audio_inputs_tensor[\"input_values\"].shape[2],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        else:\n",
    "            attention_mask_np = np.stack(normalized_attention_mask).astype(np.longlong)\n",
    "            audio_inputs_tensor[\"attention_mask\"] = torch.from_numpy(attention_mask_np)\n",
    "    else:\n",
    "        # Fallback if feature_extractor still doesn't provide attention_mask or it's problematic\n",
    "        # Correctly create a 2D attention mask (batch_size, sequence_length)\n",
    "        audio_inputs_tensor[\"attention_mask\"] = torch.ones(\n",
    "            audio_inputs_tensor[\"input_values\"].shape[0], # batch_size\n",
    "            audio_inputs_tensor[\"input_values\"].shape[2], # sequence_length\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "\n",
    "    # Process text inputs using the tokenizer\n",
    "    text_inputs = processor.tokenizer(\n",
    "        text=texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_TEXT_TOKENS,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # --- Explicitly generate audio codes for labels and decoder_input_ids ---\n",
    "    # This part needs the model's audio_encoder, so 'model' is passed to preprocess_function\n",
    "    with torch.no_grad(): # Ensure this part does not affect gradients if run inside map\n",
    "        # Move inputs to model device for encoding if not already there\n",
    "        # FIX: Remove explicit .to(torch.float16) here. Let autocast handle it.\n",
    "        input_values_for_encode = audio_inputs_tensor[\"input_values\"].to(model.device)\n",
    "        padding_mask_for_encode = audio_inputs_tensor[\"attention_mask\"].to(model.device)\n",
    "\n",
    "        # Add a check for non-finite values before passing to encoder\n",
    "        if not torch.isfinite(input_values_for_encode).all():\n",
    "            print(f\"Warning: input_values_for_encode contains non-finite values (NaN/Inf). Skipping batch.\")\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16): # Updated autocast syntax\n",
    "                audio_codes, audio_scales, _ = model.audio_encoder.encode(\n",
    "                    input_values_for_encode, # input_values_for_encode is now float32, autocast handles conversion\n",
    "                    padding_mask_for_encode, # This will now be a 2D tensor\n",
    "                    model.audio_encoder.config.target_bandwidths[-1] # Use model's audio_encoder's highest target_bandwidth config\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: An exception occurred during model.audio_encoder.encode: {e}\")\n",
    "            # Removed: print(f\"Returned value: {audio_codes}\") to prevent UnboundLocalError\n",
    "            return {} # Skip batch on encoder error\n",
    "\n",
    "        # Check if audio_codes is a tensor after encode call\n",
    "        if not isinstance(audio_codes, torch.Tensor):\n",
    "            print(f\"ERROR: model.audio_encoder.encode returned unexpected non-tensor type for audio_codes: {type(audio_codes)}\")\n",
    "            return {} # Skip batch on encoder error\n",
    "\n",
    "        # Shift audio codes to the right for training (as expected by causal LM decoder)\n",
    "        # labels are the target audio codes (unshifted)\n",
    "        # decoder_input_ids are the input to the decoder (shifted right)\n",
    "        # Ensure the last dimension is not smaller than 1 for slicing\n",
    "        if audio_codes.shape[-1] > 1:\n",
    "            labels_audio_codes = audio_codes[:, 1:]\n",
    "            decoder_input_ids_audio_codes = audio_codes[:, :-1]\n",
    "        else:\n",
    "            # Handle case where audio_codes might be too short after encoding\n",
    "            # This could result in empty labels, which Trainer won't like.\n",
    "            # For now, we'll make them identical if too short to shift.\n",
    "            print(\"Warning: Audio codes too short for shifting, setting labels and decoder_input_ids to be the same.\")\n",
    "            labels_audio_codes = audio_codes\n",
    "            decoder_input_ids_audio_codes = audio_codes\n",
    "\n",
    "        # Move labels and decoder_input_ids back to CPU if needed by dataset/trainer, or keep on device\n",
    "        labels_audio_codes = labels_audio_codes.cpu()\n",
    "        decoder_input_ids_audio_codes = decoder_input_ids_audio_codes.cpu()\n",
    "\n",
    "    # Combine the processed inputs for the model\n",
    "    # Musicgen expects audio as 'input_values' and 'padding_mask' for the audio encoder,\n",
    "    # and 'input_ids' and 'attention_mask' for the text encoder.\n",
    "    # For Trainer to work correctly, we explicitly provide 'decoder_input_ids' and 'labels' (audio codes).\n",
    "    inputs = {\n",
    "        \"input_values\": audio_inputs_tensor[\"input_values\"],    # Raw audio for encoder (can be ignored by model if decoder_input_ids are present)\n",
    "        \"padding_mask\": audio_inputs_tensor[\"attention_mask\"],   # Audio attention mask\n",
    "        \"input_ids\": text_inputs.input_ids,                      # Text input IDs for the text encoder\n",
    "        \"attention_mask\": text_inputs.attention_mask,            # Text attention mask for the text encoder\n",
    "        \"decoder_input_ids\": decoder_input_ids_audio_codes,      # Shifted audio codes for decoder input\n",
    "        \"labels\": labels_audio_codes,                            # Target audio codes for loss computation\n",
    "    }\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_loop"
   },
   "outputs": [],
   "source": [
    "# @title 5. メインループ実行\n",
    "# モデルとプロセッサの準備\n",
    "MODEL_ID = \"facebook/musicgen-large\"\n",
    "print(f\"Loading model: {MODEL_ID}...\")\n",
    "\n",
    "# 8-bit 量子化設定\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16 # A100/V100ならfloat16を推奨\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config, # 8-bit 量子化を適用\n",
    "    device_map=\"auto\" # 自動的にデバイスにマッピング\n",
    ")\n",
    "\n",
    "# PEFT (LoRA) の設定\n",
    "# MusicGenのエンコーダ (T5EncoderModel) とデコーダ (MusicgenForCausalLM) の両方にLoRAを適用\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRAのランク\n",
    "    lora_alpha=32, # LoRAスケーリング係数\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # LoRAを適用するモジュール (Attention層のQuery, Value)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\", # MusicGenはSequence-to-Sequenceモデル\n",
    ")\n",
    "\n",
    "# PEFTモデルをオリジナルモデルにアタッチ\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters() # 学習可能なパラメータ数を確認\n",
    "\n",
    "model.train()\n",
    "\n",
    "# ZIPファイルリスト取得\n",
    "zip_files = sorted(list(ZIP_DIR.glob('archive_batch_*.zip')))\n",
    "print(f\"Found {len(zip_files)} zip files.\")\n",
    "\n",
    "# 以前のチェックポイントがあればロード（簡易実装）\n",
    "latest_checkpoint_path = OUTPUT_DIR / 'latest_checkpoint'\n",
    "if latest_checkpoint_path.exists():\n",
    "    print(f\"Resuming from {latest_checkpoint_path}...\")\n",
    "    # PEFTモデルとしてロードする場合\n",
    "    model = MusicgenForConditionalGeneration.from_pretrained(\n",
    "        latest_checkpoint_path,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # LoRAアダプターをロード\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "# GPU設定 (device_map=\"auto\"を使用しているため、model.to(device)は不要)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# CUDAキャッシュをクリアしてメモリを解放\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i, zip_file in enumerate(zip_files):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing Batch {i+1}/{len(zip_files)}: {zip_file.name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # 1. 解凍\n",
    "    extract_zip(zip_file, TEMP_DATA_DIR)\n",
    "\n",
    "    # 2. メタデータ作成\n",
    "    batch_metadata_path = TEMP_WORK_DIR / 'batch.jsonl'\n",
    "    success = create_batch_metadata(METADATA_PATH, TEMP_DATA_DIR, batch_metadata_path)\n",
    "\n",
    "    if not success:\n",
    "        print(\"Skipping this batch due to metadata error.\")\n",
    "        continue\n",
    "\n",
    "    # 3. データセット準備\n",
    "    dataset = load_dataset(\"json\", data_files=str(batch_metadata_path), split=\"train\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n",
    "\n",
    "    # 前処理の適用\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    encoded_dataset = dataset.map(\n",
    "        lambda x: preprocess_function(x, processor, model), # Pass the model object here\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        batch_size=4 # メモリに応じて調整\n",
    "    )\n",
    "\n",
    "    # 4. トレーニング設定\n",
    "    # バッチごとにTrainerを作り直すが、modelは同じオブジェクトを使い回すことで学習を継続する\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(TEMP_WORK_DIR / \"results\"),\n",
    "        per_device_train_batch_size=2, # A100ならもう少し増やせるかも\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=5, # 1バッチあたりのエポック数\n",
    "        save_steps=1000, # バッチ内での保存頻度（必要なら）\n",
    "        logging_steps=10,\n",
    "        fp16=True, # A100/V100ならTrue推奨\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"wandb\", # WandB有効化\n",
    "        run_name=f\"musicgen-finetuning-batch-{i+1}\", # バッチごとにRun名を分ける\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training for this batch...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 5. モデル保存\n",
    "    # バッチ完了ごとにDriveへ保存\n",
    "    save_path = OUTPUT_DIR / f'checkpoint_batch_{i+1}'\n",
    "    print(f\"Saving model to {save_path}...\")\n",
    "    model.save_pretrained(save_path)\n",
    "    processor.save_pretrained(save_path)\n",
    "\n",
    "    # 最新版として上書き\n",
    "    latest_path = OUTPUT_DIR / 'latest_checkpoint'\n",
    "    model.save_pretrained(latest_path)\n",
    "    processor.save_pretrained(latest_path)\n",
    "\n",
    "    # 6. クリーンアップ\n",
    "    print(\"Cleaning up temp data...\")\n",
    "    shutil.rmtree(TEMP_DATA_DIR)\n",
    "    TEMP_DATA_DIR.mkdir(exist_ok=True)\n",
    "    # Trainerのクリーンアップ（メモリ解放のため）\n",
    "    del trainer\n",
    "    del dataset\n",
    "    del encoded_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"All batches processed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}