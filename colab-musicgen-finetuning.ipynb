{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c918ce9",
   "metadata": {},
   "source": [
    "# MusicGen-Large LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (A100æŽ¨å¥¨)\n",
    "\n",
    "**ç›®çš„**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç´„18,000æ›²ï¼‰ã‚’ä½¿ã„ã€MusicGen-Large ã« LoRA ã‚’é©ç”¨ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "Google Colabã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡åˆ¶é™ã‚’å›žé¿ã™ã‚‹ãŸã‚ã€**é †æ¬¡å‡¦ç†æˆ¦ç•¥ï¼ˆæŠ½å‡ºâ†’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–â†’å‰Šé™¤ï¼‰**ã‚’æŽ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ äº‹å‰æº–å‚™\n",
    "\n",
    "1. **Google Colabã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **: GPU (A100æŽ¨å¥¨) ã‚’é¸æŠž\n",
    "2. **ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®è¨­å®š**: Colabã®ã€ŒðŸ”‘ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ä»¥ä¸‹ã‚’è¨­å®š\n",
    "   - `WANDB_API_KEY`: WandB APIã‚­ãƒ¼\n",
    "   - `HF_TOKEN`: Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "3. **Google Drive**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ`archive_batch_xxxx.zip` å½¢å¼ï¼‰ã‚’é…ç½®\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ ä½¿ã„æ–¹\n",
    "\n",
    "1. ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
    "2. ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰è‡ªå‹•çš„ã«APIã‚­ãƒ¼ã‚’å–å¾—\n",
    "3. WandBã§å­¦ç¿’é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–: https://wandb.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e8f3d",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 1 â€” ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿè¡Œå‰ã«ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã® GPU ã‚’ç¢ºèªã€‚CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆã‚ã›ã¦ torch ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\n",
    "# ä¾‹ï¼šCUDA 11.8 / PyTorch 2.1 ã®å ´åˆ\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.1.0+cu118 torchaudio==2.1.0 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!apt-get update && apt-get install -y ffmpeg\n",
    "\n",
    "# audiocraft (MusicGen ã‚’å«ã‚€) å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰\n",
    "!pip install git+https://github.com/facebookresearch/audiocraft.git@main\n",
    "\n",
    "# LoRA / DeepSpeed / WandB ãªã©\n",
    "!pip install deepspeed wandb soundfile librosa loralib tqdm\n",
    "\n",
    "print(\"ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6290e",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 2 â€” Drive ãƒžã‚¦ãƒ³ãƒˆã¨è¨­å®š\n",
    "\n",
    "Google Driveã‚’ãƒžã‚¦ãƒ³ãƒˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# è¨­å®š\n",
    "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "DRIVE_ARCHIVE_DIR = '/content/drive/MyDrive/Archive_wav'\n",
    "\n",
    "# ä¸€æ™‚å±•é–‹å…ˆï¼ˆColabãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
    "TEMP_DATA_DIR = '/content/temp_dataset'\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜å…ˆï¼ˆGoogle DriveæŽ¨å¥¨ã€ã¾ãŸã¯å®¹é‡ãŒã‚ã‚Œã°ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
    "# â€»å†é–‹å¯èƒ½ã«ã™ã‚‹ãŸã‚DriveæŽ¨å¥¨ã§ã™ãŒã€I/OãŒé…ã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ã—ã¦æœ€å¾Œã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹æˆ¦ç•¥ã‚‚å¯\n",
    "# ã“ã“ã§ã¯Driveã«ç›´æŽ¥ä¿å­˜ã™ã‚‹è¨­å®šã«ã—ã¾ã™\n",
    "TOKEN_DIR = '/content/drive/MyDrive/MusicGen_Tokens'\n",
    "\n",
    "os.makedirs(TOKEN_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f'ZIPæ ¼ç´å ´æ‰€: {DRIVE_ARCHIVE_DIR}')\n",
    "print(f'ä¸€æ™‚å±•é–‹å…ˆ: {TEMP_DATA_DIR}')\n",
    "print(f'ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜å…ˆ: {TOKEN_DIR}')\n",
    "\n",
    "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "zip_files = sorted(list(Path(DRIVE_ARCHIVE_DIR).glob('archive_batch_*.zip')))\n",
    "print(f'æ¤œå‡ºã•ã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(zip_files)}')\n",
    "if len(zip_files) > 0:\n",
    "    print(f'ä¾‹: {zip_files[0].name}')\n",
    "else:\n",
    "    print('âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1cecb",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 2.5 â€” WandBã¨Hugging Faceã®èªè¨¼è¨­å®š\n",
    "\n",
    "ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df137c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—\n",
    "try:\n",
    "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    \n",
    "    # WandBãƒ­ã‚°ã‚¤ãƒ³\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    print(\"âœ“ WandBã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    print(\"âœ“ Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¾ã—ãŸ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"Colabã®ã€Œã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„:\")\n",
    "    print(\"  - WANDB_API_KEY: WandBã®APIã‚­ãƒ¼\")\n",
    "    print(\"  - HF_TOKEN: Hugging Faceã®APIãƒˆãƒ¼ã‚¯ãƒ³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e1451",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 2.6 â€” ðŸ§ª å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ã®å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "æœ€åˆã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å°‘æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®å‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "from audiocraft.models import CompressionModel, MusicGen\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import loralib as lora\n",
    "import torch.nn as nn\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆè¨­å®š\n",
    "TEST_MODE = True\n",
    "NUM_TEST_SAMPLES = 5\n",
    "TEST_EPOCHS = 2\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ§ª å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "TEST_DIR = '/content/test_workspace'\n",
    "TEST_WAV_DIR = os.path.join(TEST_DIR, 'wavs')\n",
    "TEST_TOKEN_DIR = os.path.join(TEST_DIR, 'tokens')\n",
    "os.makedirs(TEST_WAV_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_TOKEN_DIR, exist_ok=True)\n",
    "\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆæœ€åˆã®ZIPã‹ã‚‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ï¼‰\n",
    "print(\"ðŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\")\n",
    "if len(zip_files) > 0:\n",
    "    target_zip = zip_files[0]\n",
    "    print(f\"  ZIPãƒ•ã‚¡ã‚¤ãƒ«: {target_zip.name}\")\n",
    "    # ä¸€æ™‚çš„ã«å±•é–‹ã—ã¦æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚³ãƒ”ãƒ¼\n",
    "    !mkdir -p /content/temp_extract\n",
    "    !tar -xzf \"{target_zip}\" -C /content/temp_extract\n",
    "    \n",
    "    extracted_files = list(Path('/content/temp_extract').rglob('*.wav'))\n",
    "    print(f\"  æŠ½å‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(extracted_files)}\")\n",
    "    \n",
    "    for i, f in enumerate(extracted_files[:NUM_TEST_SAMPLES]):\n",
    "        shutil.copy(f, TEST_WAV_DIR)\n",
    "        print(f\"  ã‚³ãƒ”ãƒ¼: {f.name}\")\n",
    "    \n",
    "    # æŽƒé™¤\n",
    "    !rm -rf /content/temp_extract\n",
    "else:\n",
    "    print(\"âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™\")\n",
    "    # ãƒ€ãƒŸãƒ¼WAVä½œæˆï¼ˆæ­£å¼¦æ³¢ï¼‰\n",
    "    sr = 32000\n",
    "    for i in range(NUM_TEST_SAMPLES):\n",
    "        t = torch.linspace(0, 5, sr * 5)\n",
    "        wav = torch.sin(2 * 3.14159 * 440 * t).unsqueeze(0)\n",
    "        torchaudio.save(os.path.join(TEST_WAV_DIR, f'test_{i}.wav'), wav, sr)\n",
    "\n",
    "test_paths = list(Path(TEST_WAV_DIR).glob('*.wav'))\n",
    "print(f\"âœ“ {len(test_paths)}å€‹ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„\")\n",
    "print()\n",
    "\n",
    "# 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ðŸŽµ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®ãƒ†ã‚¹ãƒˆ\")\n",
    "compression_model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
    "compression_model.to('cuda')\n",
    "compression_model.eval()\n",
    "\n",
    "for i, p in enumerate(test_paths):\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(p)\n",
    "        if sr != 32000:\n",
    "            resampler = torchaudio.transforms.Resample(sr, 32000)\n",
    "            wav = resampler(wav)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        wav = wav.unsqueeze(0).to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded_frames = compression_model.encode(wav)\n",
    "            tokens = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
    "        \n",
    "        outp = os.path.join(TEST_TOKEN_DIR, p.stem + '.pt')\n",
    "        torch.save({'tokens': tokens.cpu(), 'path': str(p)}, outp)\n",
    "        print(f\"  âœ“ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†: {p.name} -> {tokens.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "del compression_model\n",
    "torch.cuda.empty_cache()\n",
    "print()\n",
    "\n",
    "# 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ðŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "class TestTokenDataset(Dataset):\n",
    "    def __init__(self, token_dir, max_length=1500):\n",
    "        self.files = sorted(list(Path(token_dir).glob('*.pt')))\n",
    "        self.max_length = max_length\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx])\n",
    "        tokens = data['tokens']\n",
    "        if tokens.dim() == 3: tokens = tokens.squeeze(0)\n",
    "        seq_len = tokens.shape[-1]\n",
    "        if seq_len > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        elif seq_len < self.max_length:\n",
    "            pad = self.max_length - seq_len\n",
    "            tokens = torch.nn.functional.pad(tokens, (0, pad), value=0)\n",
    "        return tokens\n",
    "\n",
    "test_dataset = TestTokenDataset(TEST_TOKEN_DIR)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "for batch in test_loader:\n",
    "    print(f\"âœ“ ãƒãƒƒãƒå½¢çŠ¶: {batch.shape}\")\n",
    "    break\n",
    "print()\n",
    "\n",
    "# 4. ãƒ¢ãƒ‡ãƒ«ã¨LoRAã®ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ðŸ¤– ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨LoRAé©ç”¨ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-large')\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "def apply_lora_to_linear(module, r=8, lora_alpha=32, lora_dropout=0.1, parent_name='', count=[0]):\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.Linear) and count[0] < 3:  # ãƒ†ã‚¹ãƒˆç”¨åˆ¶é™\n",
    "            lora_linear = lora.Linear(child.in_features, child.out_features, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=(child.bias is not None))\n",
    "            lora_linear.weight.data = child.weight.data.clone()\n",
    "            if child.bias is not None: lora_linear.bias.data = child.bias.data.clone()\n",
    "            setattr(module, name, lora_linear)\n",
    "            count[0] += 1\n",
    "            print(f\"  âœ“ LoRAé©ç”¨: {parent_name}.{name}\")\n",
    "        else:\n",
    "            apply_lora_to_linear(child, r, lora_alpha, lora_dropout, f\"{parent_name}.{name}\" if parent_name else name, count)\n",
    "\n",
    "apply_lora_to_linear(model.lm, r=8)\n",
    "print(\"âœ“ LoRAé©ç”¨å®Œäº†\")\n",
    "print()\n",
    "\n",
    "# 5. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ðŸ‹ï¸ ã‚¹ãƒ†ãƒƒãƒ—5: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "model.train()\n",
    "for epoch in range(TEST_EPOCHS):\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to('cuda')\n",
    "        out = model.lm.forward(batch)\n",
    "        loss = criterion(out, batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"  Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "        break\n",
    "print()\n",
    "\n",
    "# 6. ç”Ÿæˆãƒ†ã‚¹ãƒˆ\n",
    "print(\"ðŸŽ¶ ã‚¹ãƒ†ãƒƒãƒ—6: ç”Ÿæˆãƒ†ã‚¹ãƒˆ\")\n",
    "model.eval()\n",
    "model.set_generation_params(duration=5)\n",
    "with torch.no_grad():\n",
    "    wav = model.generate([\"Test melody\"])\n",
    "print(f\"âœ“ ç”Ÿæˆå®Œäº†: {wav.shape}\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf0992",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 3 â€” é †æ¬¡å‡¦ç†ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º (Sequential Tokenization)\n",
    "\n",
    "**æˆ¦ç•¥**: ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’1ã¤ãšã¤å±•é–‹ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ WAVå‰Šé™¤ ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚\n",
    "ã“ã‚Œã«ã‚ˆã‚Šã€Colabã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’åœ§è¿«ã›ãšã«å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de228b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from audiocraft.models import CompressionModel\n",
    "import wandb\n",
    "\n",
    "# WandBåˆæœŸåŒ–\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "    if not wandb.api.api_key: wandb.login(key=WANDB_API_KEY)\n",
    "    wandb.init(project='musicgen-lora-finetune', name='sequential_tokenization', job_type='preprocessing')\n",
    "    use_wandb = True\n",
    "except: use_wandb = False\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
    "print('EnCodecãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...')\n",
    "compression_model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
    "compression_model.to('cuda')\n",
    "compression_model.eval()\n",
    "\n",
    "# å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "existing_tokens = set([Path(f).stem for f in glob.glob(os.path.join(TOKEN_DIR, '*.pt'))])\n",
    "print(f'æ—¢ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿: {len(existing_tokens)} ãƒ•ã‚¡ã‚¤ãƒ«')\n",
    "\n",
    "total_processed = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ãƒ«ãƒ¼ãƒ—\n",
    "for zip_idx, zip_file in enumerate(tqdm(zip_files, desc='ZIPãƒãƒƒãƒå‡¦ç†')):\n",
    "    print(f'\\nðŸ“¦ å‡¦ç†ä¸­: {zip_file.name} ({zip_idx+1}/{len(zip_files)})')\n",
    "    \n",
    "    # 1. å±•é–‹ (Extract)\n",
    "    if os.path.exists(TEMP_DATA_DIR):\n",
    "        shutil.rmtree(TEMP_DATA_DIR)\n",
    "    os.makedirs(TEMP_DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    print('  â†³ å±•é–‹ä¸­...')\n",
    "    !tar -xzf \"{zip_file}\" -C \"{TEMP_DATA_DIR}\"\n",
    "    \n",
    "    wav_files = list(Path(TEMP_DATA_DIR).rglob('*.wav'))\n",
    "    print(f'  â†³ {len(wav_files)} å€‹ã®WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º')\n",
    "    \n",
    "    # 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (Tokenize)\n",
    "    batch_processed = 0\n",
    "    for wav_path in tqdm(wav_files, desc='  â†³ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–', leave=False):\n",
    "        if wav_path.stem in existing_tokens:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            wav, sr = torchaudio.load(str(wav_path))\n",
    "            if sr != 32000:\n",
    "                resampler = torchaudio.transforms.Resample(sr, 32000)\n",
    "                wav = resampler(wav)\n",
    "            if wav.shape[0] > 1:\n",
    "                wav = wav.mean(dim=0, keepdim=True)\n",
    "            wav = wav.unsqueeze(0).to('cuda')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                encoded_frames = compression_model.encode(wav)\n",
    "                tokens = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
    "            \n",
    "            outp = os.path.join(TOKEN_DIR, wav_path.stem + '.pt')\n",
    "            torch.save({'tokens': tokens.cpu(), 'path': str(wav_path)}, outp)\n",
    "            \n",
    "            batch_processed += 1\n",
    "            total_processed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error: {wav_path.name} - {e}')\n",
    "            continue\n",
    "            \n",
    "    # 3. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Cleanup)\n",
    "    print('  â†³ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...')\n",
    "    shutil.rmtree(TEMP_DATA_DIR)\n",
    "    \n",
    "    # ãƒ­ã‚°\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'processed_files': total_processed,\n",
    "            'processed_zips': zip_idx + 1,\n",
    "            'files_in_batch': batch_processed\n",
    "        })\n",
    "\n",
    "print(f'\\nâœ… å…¨å‡¦ç†å®Œäº†: {total_processed} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã—ãŸ')\n",
    "if use_wandb: wandb.finish()\n",
    "\n",
    "del compression_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3f7af",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 4 â€” DeepSpeed è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b74acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ds_config = {\n",
    "  \"train_batch_size\": 16,\n",
    "  \"train_micro_batch_size_per_gpu\": 2,\n",
    "  \"gradient_accumulation_steps\": 8,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": 1e-4,\n",
    "      \"weight_decay\": 0.01\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": True\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": True\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\"\n",
    "    },\n",
    "    \"overlap_comm\": True,\n",
    "    \"contiguous_gradients\": True\n",
    "  }\n",
    "}\n",
    "\n",
    "with open('ds_config.json','w') as f:\n",
    "    json.dump(ds_config, f, indent=2)\n",
    "    \n",
    "print('ds_config.json ã‚’ä½œæˆã—ã¾ã—ãŸ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967cb7b",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 5 â€” ãƒ¢ãƒ‡ãƒ«æº–å‚™ã¨LoRAé©ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from audiocraft.models import MusicGen\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
    "print('MusicGen-Large ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...')\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-large')\n",
    "model.eval()\n",
    "\n",
    "# LoRAé©ç”¨é–¢æ•°\n",
    "def apply_lora_to_linear(module, r=8, lora_alpha=32, lora_dropout=0.1, parent_name=''):\n",
    "    for name, child in list(module.named_children()):\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        if isinstance(child, nn.Linear):\n",
    "            lora_linear = lora.Linear(\n",
    "                child.in_features, child.out_features, r=r, \n",
    "                lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=(child.bias is not None)\n",
    "            )\n",
    "            lora_linear.weight.data = child.weight.data.clone()\n",
    "            if child.bias is not None:\n",
    "                lora_linear.bias.data = child.bias.data.clone()\n",
    "            setattr(module, name, lora_linear)\n",
    "            print(f'Applied LoRA to: {full_name}')\n",
    "        else:\n",
    "            apply_lora_to_linear(child, r, lora_alpha, lora_dropout, full_name)\n",
    "\n",
    "print('\\nLoRAã‚’é©ç”¨ä¸­...')\n",
    "apply_lora_to_linear(model.lm, r=8)\n",
    "\n",
    "# å‹¾é…è¨­å®š\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for name, p in model.named_parameters():\n",
    "    if 'lora_' in name: p.requires_grad = True\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'\\nå­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41971f7b",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 6 â€” ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba719a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, token_dir, max_length=1500):\n",
    "        self.files = sorted(list(Path(token_dir).glob('*.pt')))\n",
    "        self.max_length = max_length\n",
    "        print(f'Dataset initialized with {len(self.files)} samples')\n",
    "        \n",
    "    def __len__(self): return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx])\n",
    "        tokens = data['tokens']\n",
    "        if tokens.dim() == 3: tokens = tokens.squeeze(0)\n",
    "        \n",
    "        seq_len = tokens.shape[-1]\n",
    "        if seq_len > self.max_length:\n",
    "            start = random.randint(0, seq_len - self.max_length)\n",
    "            tokens = tokens[:, start:start + self.max_length]\n",
    "        elif seq_len < self.max_length:\n",
    "            pad = self.max_length - seq_len\n",
    "            tokens = torch.nn.functional.pad(tokens, (0, pad), value=0)\n",
    "        return tokens\n",
    "\n",
    "dataset = TokenDataset(TOKEN_DIR, max_length=1500)\n",
    "train_size = int(0.95 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Valid: {len(valid_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a627b",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 7 â€” å­¦ç¿’ãƒ«ãƒ¼ãƒ— (WandB + Checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47016277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from google.colab import userdata\n",
    "\n",
    "# WandBåˆæœŸåŒ–\n",
    "try:\n",
    "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "    if not wandb.api.api_key: wandb.login(key=WANDB_API_KEY)\n",
    "    wandb.init(project='musicgen-lora-finetune', name='training_run', tags=['musicgen', 'lora', 'a100'])\n",
    "    use_wandb = True\n",
    "except: use_wandb = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# ä¿å­˜å…ˆè¨­å®š\n",
    "CKPT_DIR = '/content/drive/MyDrive/MusicGen_Checkpoints'\n",
    "LORA_DIR = os.path.join(CKPT_DIR, 'lora_weights')\n",
    "FULL_MODEL_DIR = os.path.join(CKPT_DIR, 'full_models')\n",
    "os.makedirs(LORA_DIR, exist_ok=True)\n",
    "os.makedirs(FULL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def extract_lora_weights(model):\n",
    "    return {name: param.cpu().detach().clone() for name, param in model.named_parameters() if 'lora_' in name and param.requires_grad}\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, global_step, loss, checkpoint_type='regular'):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch, 'global_step': global_step, 'model_state': model.state_dict(),\n",
    "        'opt_state': optimizer.state_dict(), 'loss': loss, 'best_loss': best_loss,\n",
    "        'timestamp': time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    }\n",
    "    lora_checkpoint = {\n",
    "        'epoch': epoch, 'global_step': global_step, 'lora_weights': extract_lora_weights(model),\n",
    "        'loss': loss, 'timestamp': checkpoint['timestamp']\n",
    "    }\n",
    "    \n",
    "    if checkpoint_type == 'regular':\n",
    "        torch.save(checkpoint, os.path.join(CKPT_DIR, 'latest.pt'))\n",
    "        torch.save(lora_checkpoint, os.path.join(LORA_DIR, 'latest_lora.pt'))\n",
    "    elif checkpoint_type == 'best':\n",
    "        torch.save(checkpoint, os.path.join(FULL_MODEL_DIR, 'best_model.pt'))\n",
    "        torch.save(lora_checkpoint, os.path.join(LORA_DIR, 'best_lora.pt'))\n",
    "        print(f'  ðŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: Loss {loss:.4f}')\n",
    "    elif checkpoint_type == 'epoch':\n",
    "        torch.save(checkpoint, os.path.join(FULL_MODEL_DIR, f'checkpoint_epoch_{epoch+1}.pt'))\n",
    "        torch.save(lora_checkpoint, os.path.join(LORA_DIR, f'lora_epoch_{epoch+1}.pt'))\n",
    "    elif checkpoint_type == 'step':\n",
    "        torch.save(checkpoint, os.path.join(CKPT_DIR, f'checkpoint_step_{global_step}.pt'))\n",
    "\n",
    "# å†é–‹å‡¦ç†\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "latest_ckpt = os.path.join(CKPT_DIR, 'latest.pt')\n",
    "if os.path.exists(latest_ckpt):\n",
    "    print(f'ðŸ”„ å†é–‹: {latest_ckpt}')\n",
    "    ckpt = torch.load(latest_ckpt, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model_state'], strict=False)\n",
    "    optimizer.load_state_dict(ckpt['opt_state'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    global_step = ckpt.get('global_step', 0)\n",
    "    best_loss = ckpt.get('best_loss', float('inf'))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 5\n",
    "accum_steps = 8\n",
    "\n",
    "print('\\nðŸš€ å­¦ç¿’é–‹å§‹')\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_idx, tokens in enumerate(pbar):\n",
    "        try:\n",
    "            tokens = tokens.to(device)\n",
    "            outputs = model.lm.forward(tokens)\n",
    "            loss = criterion(outputs, tokens.float()) / accum_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                if use_wandb and global_step % 10 == 0:\n",
    "                    wandb.log({'train/loss': loss.item() * accum_steps, 'train/step': global_step})\n",
    "                if global_step % 500 == 0:\n",
    "                    save_checkpoint(model, optimizer, epoch, global_step, loss.item() * accum_steps, 'step')\n",
    "            \n",
    "            epoch_loss += loss.item() * accum_steps\n",
    "            pbar.set_postfix({'loss': f'{loss.item() * accum_steps:.4f}'})\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            continue\n",
    "            \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'\\nEpoch {epoch+1} Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    if use_wandb: wandb.log({'epoch/loss': avg_loss, 'epoch': epoch+1})\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_checkpoint(model, optimizer, epoch, global_step, avg_loss, 'best')\n",
    "    \n",
    "    save_checkpoint(model, optimizer, epoch, global_step, avg_loss, 'regular')\n",
    "    save_checkpoint(model, optimizer, epoch, global_step, avg_loss, 'epoch')\n",
    "\n",
    "print('ðŸŽ‰ å­¦ç¿’å®Œäº†')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353a749",
   "metadata": {},
   "source": [
    "# ã‚»ãƒ« 8 â€” æ¥½æ›²ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e68fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.data.audio import audio_write\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print('ðŸŽµ æ¥½æ›²ç”Ÿæˆ')\n",
    "model.eval()\n",
    "model.set_generation_params(use_sampling=True, top_k=250, duration=30)\n",
    "\n",
    "prompts = [\n",
    "    \"A dynamic heavy metal song with fast drums and guitar solo\",\n",
    "    \"Relaxing jazz piano with soft background ambience\",\n",
    "    \"Upbeat electronic dance music with strong bass\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    wav = model.generate(prompts)\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/MusicGen_Generated'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, one_wav in enumerate(wav):\n",
    "    filename = os.path.join(output_dir, f\"generated_{idx}\")\n",
    "    audio_write(filename, one_wav.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
    "    print(f'\\nPrompt: {prompts[idx]}')\n",
    "    display(Audio(filename + \".wav\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
