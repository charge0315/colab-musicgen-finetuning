{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c918ce9",
      "metadata": {
        "id": "5c918ce9"
      },
      "source": [
        "# MusicGen-Large LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (A100æŽ¨å¥¨)\n",
        "\n",
        "**ç›®çš„**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç´„18,000æ›²ï¼‰ã‚’ä½¿ã„ã€MusicGen-Large ã« LoRA ã‚’é©ç”¨ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚\n",
        "Google Colabã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡åˆ¶é™ã‚’å›žé¿ã™ã‚‹ãŸã‚ã€**é †æ¬¡å‡¦ç†æˆ¦ç•¥ï¼ˆæŠ½å‡ºâ†’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–â†’å‰Šé™¤ï¼‰**ã‚’æŽ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ äº‹å‰æº–å‚™\n",
        "\n",
        "1. **Google Colabã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **: GPU (A100æŽ¨å¥¨) ã‚’é¸æŠž\n",
        "2. **ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®è¨­å®š**: Colabã®ã€ŒðŸ”‘ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ä»¥ä¸‹ã‚’è¨­å®š\n",
        "   - `WANDB_API_KEY`: WandB APIã‚­ãƒ¼\n",
        "   - `HF_TOKEN`: Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³\n",
        "3. **Google Drive**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ`archive_batch_xxxx.zip` å½¢å¼ï¼‰ã‚’é…ç½®\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ ä½¿ã„æ–¹\n",
        "\n",
        "1. ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n",
        "2. ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰è‡ªå‹•çš„ã«APIã‚­ãƒ¼ã‚’å–å¾—\n",
        "3. WandBã§å­¦ç¿’é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–: https://wandb.ai/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6e8f3d",
      "metadata": {
        "id": "ff6e8f3d"
      },
      "source": [
        "# ã‚»ãƒ« 1 â€” ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hoPXeER8ZYvm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoPXeER8ZYvm",
        "outputId": "1cf759eb-758f-4f99-fc36-935ee79d3a42"
      },
      "outputs": [],
      "source": [
        "# @title fix environment - must restart notebook after!!\n",
        "ROOT_DIR = f'/content'\n",
        "\n",
        "%cd {ROOT_DIR}\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install -q timm==1.0.22\n",
        "!pip install -q fastai==2.8.5\n",
        "!pip install -q torch==2.9.0\n",
        "!pip install -q torchaudio==2.9.0\n",
        "!pip install -q torchtext==0.6.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q torchvision==0.24.0\n",
        "\n",
        "!pip install -q triton==3.5.0\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchtext\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(torchtext.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "!pip install -q xformers==0.0.29.post1 --index-url https://download.pytorch.org/whl/cu121 # cuda 12.1 version\n",
        "\n",
        "!run -m xformers.info\n",
        "!python os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "# ^^^ @baltigor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bemXRN_8Zgrh",
      "metadata": {
        "id": "bemXRN_8Zgrh"
      },
      "outputs": [],
      "source": [
        "# @title download and install\n",
        "\n",
        "import os\n",
        "ROOT_DIR = '/content'\n",
        "AUDIOCRAFT_DIR = os.path.join(ROOT_DIR, 'audiocraft')\n",
        "AUDIOCRAFT_REPO = 'https://github.com/facebookresearch/audiocraft.git'\n",
        "\n",
        "# audiocraftãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤\n",
        "if os.path.exists(AUDIOCRAFT_DIR):\n",
        "    import shutil\n",
        "    shutil.rmtree(AUDIOCRAFT_DIR)\n",
        "\n",
        "# audiocraftã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
        "!git clone {AUDIOCRAFT_REPO} {AUDIOCRAFT_DIR}\n",
        "\n",
        "# audiocraftãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "%cd {AUDIOCRAFT_DIR}\n",
        "!pip install -q -e .\n",
        "\n",
        "# ãã®ä»–ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install -q dora-search numba wandb loralib\n",
        "\n",
        "# ^^^ @baltigor\n",
        "\n",
        "!python -m pip install -U pip setuptools wheel\n",
        "!python -m pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz\n",
        "\n",
        "!python -m pip install laion-clap\n",
        "!python -m pip install -U transformers==4.30.0\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª\n",
        "print(\"âœ“ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
        "try:\n",
        "    from audiocraft.models import CompressionModel, MusicGen\n",
        "    print(\"âœ“ audiocraft.models ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    print(\"ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  > ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d6290e",
      "metadata": {
        "id": "11d6290e"
      },
      "source": [
        "# ã‚»ãƒ« 2 â€” Drive ãƒžã‚¦ãƒ³ãƒˆã¨è¨­å®š\n",
        "\n",
        "Google Driveã‚’ãƒžã‚¦ãƒ³ãƒˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53bd8d67",
      "metadata": {
        "id": "53bd8d67"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# è¨­å®š\n",
        "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "DRIVE_ARCHIVE_DIR = '/content/drive/MyDrive/Archive_Wavs'\n",
        "\n",
        "# ä¸€æ™‚å±•é–‹å…ˆï¼ˆColabãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
        "TEMP_DATA_DIR = '/content/temp_dataset'\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜å…ˆï¼ˆGoogle DriveæŽ¨å¥¨ã€ã¾ãŸã¯å®¹é‡ãŒã‚ã‚Œã°ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
        "# â€»å†é–‹å¯èƒ½ã«ã™ã‚‹ãŸã‚DriveæŽ¨å¥¨ã§ã™ãŒã€I/OãŒé…ã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ã—ã¦æœ€å¾Œã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹æˆ¦ç•¥ã‚‚å¯\n",
        "# ã“ã“ã§ã¯Driveã«ç›´æŽ¥ä¿å­˜ã™ã‚‹è¨­å®šã«ã—ã¾ã™\n",
        "TOKEN_DIR = '/content/drive/MyDrive/MusicGen_Tokens'\n",
        "\n",
        "os.makedirs(TOKEN_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f'ZIPæ ¼ç´å ´æ‰€: {DRIVE_ARCHIVE_DIR}')\n",
        "print(f'ä¸€æ™‚å±•é–‹å…ˆ: {TEMP_DATA_DIR}')\n",
        "print(f'ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜å…ˆ: {TOKEN_DIR}')\n",
        "\n",
        "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
        "zip_files = sorted(list(Path(DRIVE_ARCHIVE_DIR).glob('archive_batch_*.zip')))\n",
        "print(f'æ¤œå‡ºã•ã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(zip_files)}')\n",
        "if len(zip_files) > 0:\n",
        "    print(f'ä¾‹: {zip_files[0].name}')\n",
        "else:\n",
        "    print('âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b1cecb",
      "metadata": {
        "id": "d8b1cecb"
      },
      "source": [
        "# ã‚»ãƒ« 2.5 â€” WandBã¨Hugging Faceã®èªè¨¼è¨­å®š\n",
        "\n",
        "ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df137c8",
      "metadata": {
        "id": "0df137c8"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—\n",
        "try:\n",
        "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # WandBãƒ­ã‚°ã‚¤ãƒ³\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "    print(\"âœ“ WandBã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ\")\n",
        "\n",
        "    # Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š\n",
        "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    print(\"âœ“ Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¾ã—ãŸ\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    print(\"Colabã®ã€Œã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„:\")\n",
        "    print(\"  - WANDB_API_KEY: WandBã®APIã‚­ãƒ¼\")\n",
        "    print(\"  - HF_TOKEN: Hugging Faceã®APIãƒˆãƒ¼ã‚¯ãƒ³\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6e1451",
      "metadata": {
        "id": "1e6e1451"
      },
      "source": [
        "# ã‚»ãƒ« 2.6 â€” ðŸ§ª å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ã®å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ\n",
        "\n",
        "æœ€åˆã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å°‘æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®å‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3cdaf43",
      "metadata": {
        "id": "a3cdaf43"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from audiocraft.models import CompressionModel, MusicGen\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import loralib as lora\n",
        "import torch.nn as nn\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆè¨­å®š\n",
        "TEST_MODE = True\n",
        "NUM_TEST_SAMPLES = 5\n",
        "TEST_EPOCHS = 2\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ§ª å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "TEST_DIR = '/content/test_workspace'\n",
        "TEST_WAV_DIR = os.path.join(TEST_DIR, 'wavs')\n",
        "TEST_TOKEN_DIR = os.path.join(TEST_DIR, 'tokens')\n",
        "os.makedirs(TEST_WAV_DIR, exist_ok=True)\n",
        "os.makedirs(TEST_TOKEN_DIR, exist_ok=True)\n",
        "\n",
        "# 1. ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆæœ€åˆã®ZIPã‹ã‚‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ï¼‰\n",
        "print(\"ðŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\")\n",
        "if len(zip_files) > 0:\n",
        "    target_zip = zip_files[0]\n",
        "    print(f\"  ZIPãƒ•ã‚¡ã‚¤ãƒ«: {target_zip.name}\")\n",
        "    # ä¸€æ™‚çš„ã«å±•é–‹ã—ã¦æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚³ãƒ”ãƒ¼\n",
        "    !mkdir -p /content/temp_extract\n",
        "    !tar -xzf \"{target_zip}\" -C /content/temp_extract\n",
        "\n",
        "    extracted_files = list(Path('/content/temp_extract').rglob('*.wav'))\n",
        "    print(f\"  æŠ½å‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(extracted_files)}\")\n",
        "\n",
        "    for i, f in enumerate(extracted_files[:NUM_TEST_SAMPLES]):\n",
        "        shutil.copy(f, TEST_WAV_DIR)\n",
        "        print(f\"  ã‚³ãƒ”ãƒ¼: {f.name}\")\n",
        "\n",
        "    # æŽƒé™¤\n",
        "    !rm -rf /content/temp_extract\n",
        "else:\n",
        "    print(\"âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™\")\n",
        "    # ãƒ€ãƒŸãƒ¼WAVä½œæˆï¼ˆæ­£å¼¦æ³¢ï¼‰\n",
        "    sr = 32000\n",
        "    for i in range(NUM_TEST_SAMPLES):\n",
        "        t = torch.linspace(0, 5, sr * 5)\n",
        "        wav = torch.sin(2 * 3.14159 * 440 * t).unsqueeze(0)\n",
        "        torchaudio.save(os.path.join(TEST_WAV_DIR, f'test_{i}.wav'), wav, sr)\n",
        "\n",
        "test_paths = list(Path(TEST_WAV_DIR).glob('*.wav'))\n",
        "print(f\"âœ“ {len(test_paths)}å€‹ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„\")\n",
        "print()\n",
        "\n",
        "# 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®ãƒ†ã‚¹ãƒˆ\n",
        "print(\"ðŸŽµ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®ãƒ†ã‚¹ãƒˆ\")\n",
        "compression_model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
        "compression_model.to('cuda')\n",
        "compression_model.eval()\n",
        "\n",
        "for i, p in enumerate(test_paths):\n",
        "    try:\n",
        "        wav, sr = torchaudio.load(p)\n",
        "        if sr != 32000:\n",
        "            resampler = torchaudio.transforms.Resample(sr, 32000)\n",
        "            wav = resampler(wav)\n",
        "        if wav.shape[0] > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "        wav = wav.unsqueeze(0).to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded_frames = compression_model.encode(wav)\n",
        "            tokens = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
        "\n",
        "        outp = os.path.join(TEST_TOKEN_DIR, p.stem + '.pt')\n",
        "        torch.save({'tokens': tokens.cpu(), 'path': str(p)}, outp)\n",
        "        print(f\"  âœ“ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†: {p.name} -> {tokens.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "\n",
        "del compression_model\n",
        "torch.cuda.empty_cache()\n",
        "print()\n",
        "\n",
        "# 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ãƒ†ã‚¹ãƒˆ\n",
        "print(\"ðŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ãƒ†ã‚¹ãƒˆ\")\n",
        "class TestTokenDataset(Dataset):\n",
        "    def __init__(self, token_dir, max_length=1500):\n",
        "        self.files = sorted(list(Path(token_dir).glob('*.pt')))\n",
        "        self.max_length = max_length\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.load(self.files[idx])\n",
        "        tokens = data['tokens']\n",
        "        if tokens.dim() == 3: tokens = tokens.squeeze(0)\n",
        "        seq_len = tokens.shape[-1]\n",
        "        if seq_len > self.max_length:\n",
        "            tokens = tokens[:, :self.max_length]\n",
        "        elif seq_len < self.max_length:\n",
        "            pad = self.max_length - seq_len\n",
        "            tokens = torch.nn.functional.pad(tokens, (0, pad), value=0)\n",
        "        return tokens\n",
        "\n",
        "test_dataset = TestTokenDataset(TEST_TOKEN_DIR)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
        "for batch in test_loader:\n",
        "    print(f\"âœ“ ãƒãƒƒãƒå½¢çŠ¶: {batch.shape}\")\n",
        "    break\n",
        "print()\n",
        "\n",
        "# 4. ãƒ¢ãƒ‡ãƒ«ã¨LoRAã®ãƒ†ã‚¹ãƒˆ\n",
        "print(\"ðŸ¤– ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨LoRAé©ç”¨ã®ãƒ†ã‚¹ãƒˆ\")\n",
        "model = MusicGen.get_pretrained('facebook/musicgen-large')\n",
        "model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "def apply_lora_to_linear(module, r=8, lora_alpha=32, lora_dropout=0.1, parent_name='', count=[0]):\n",
        "    for name, child in list(module.named_children()):\n",
        "        if isinstance(child, nn.Linear) and count[0] < 3:  # ãƒ†ã‚¹ãƒˆç”¨åˆ¶é™\n",
        "            lora_linear = lora.Linear(child.in_features, child.out_features, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=(child.bias is not None))\n",
        "            lora_linear.weight.data = child.weight.data.clone()\n",
        "            if child.bias is not None: lora_linear.bias.data = child.bias.data.clone()\n",
        "            setattr(module, name, lora_linear)\n",
        "            count[0] += 1\n",
        "            print(f\"  âœ“ LoRAé©ç”¨: {parent_name}.{name}\")\n",
        "        else:\n",
        "            apply_lora_to_linear(child, r, lora_alpha, lora_dropout, f\"{parent_name}.{name}\" if parent_name else name, count)\n",
        "\n",
        "apply_lora_to_linear(model.lm, r=8)\n",
        "print(\"âœ“ LoRAé©ç”¨å®Œäº†\")\n",
        "print()\n",
        "\n",
        "# 5. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ãƒ†ã‚¹ãƒˆ\n",
        "print(\"ðŸ‹ï¸ ã‚¹ãƒ†ãƒƒãƒ—5: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ãƒ†ã‚¹ãƒˆ\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "model.train()\n",
        "for epoch in range(TEST_EPOCHS):\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to('cuda')\n",
        "        out = model.lm.forward(batch)\n",
        "        loss = criterion(out, batch.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"  Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "        break\n",
        "print()\n",
        "\n",
        "# 6. ç”Ÿæˆãƒ†ã‚¹ãƒˆ\n",
        "print(\"ðŸŽ¶ ã‚¹ãƒ†ãƒƒãƒ—6: ç”Ÿæˆãƒ†ã‚¹ãƒˆ\")\n",
        "model.eval()\n",
        "model.set_generation_params(duration=5)\n",
        "with torch.no_grad():\n",
        "    wav = model.generate([\"Test melody\"])\n",
        "print(f\"âœ“ ç”Ÿæˆå®Œäº†: {wav.shape}\")\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"âœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9356c1b1",
      "metadata": {},
      "source": [
        "# ã‚»ãƒ« 3.5 â€” ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé€Ÿåº¦è¨ˆæ¸¬\n",
        "\n",
        "æœ€åˆã®100ãƒ•ã‚¡ã‚¤ãƒ«ã§å‡¦ç†é€Ÿåº¦ã‚’è¨ˆæ¸¬ã—ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†æ™‚é–“ã‚’æŽ¨å®šã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f185b05",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "from audiocraft.models import CompressionModel\n",
        "\n",
        "print(\"â±ï¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé€Ÿåº¦è¨ˆæ¸¬\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
        "compression_model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
        "compression_model.to('cuda')\n",
        "compression_model.eval()\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "NUM_SAMPLES = 100\n",
        "\n",
        "# æœ€åˆã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º\n",
        "if len(zip_files) > 0:\n",
        "    target_zip = zip_files[0]\n",
        "    print(f\"ðŸ“¦ ãƒ†ã‚¹ãƒˆZIP: {target_zip.name}\")\n",
        "    \n",
        "    # ä¸€æ™‚å±•é–‹\n",
        "    test_extract_dir = '/content/temp_speed_test'\n",
        "    if os.path.exists(test_extract_dir):\n",
        "        shutil.rmtree(test_extract_dir)\n",
        "    os.makedirs(test_extract_dir, exist_ok=True)\n",
        "    \n",
        "    !tar -xzf \"{target_zip}\" -C \"{test_extract_dir}\"\n",
        "    \n",
        "    wav_files = list(Path(test_extract_dir).rglob('*.wav'))[:NUM_SAMPLES]\n",
        "    print(f\"ðŸ“Š ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(wav_files)}\")\n",
        "    \n",
        "    # é€Ÿåº¦è¨ˆæ¸¬\n",
        "    start_time = time.time()\n",
        "    processed = 0\n",
        "    \n",
        "    for wav_path in tqdm(wav_files, desc=\"è¨ˆæ¸¬ä¸­\"):\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(str(wav_path))\n",
        "            if sr != 32000:\n",
        "                resampler = torchaudio.transforms.Resample(sr, 32000)\n",
        "                wav = resampler(wav)\n",
        "            if wav.shape[0] > 1:\n",
        "                wav = wav.mean(dim=0, keepdim=True)\n",
        "            wav = wav.unsqueeze(0).to('cuda')\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                encoded_frames = compression_model.encode(wav)\n",
        "                tokens = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
        "            \n",
        "            processed += 1\n",
        "        except Exception as e:\n",
        "            print(f\"ã‚¨ãƒ©ãƒ¼: {wav_path.name} - {e}\")\n",
        "            continue\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    files_per_sec = processed / elapsed if elapsed > 0 else 0\n",
        "    \n",
        "    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®æŽ¨å®š\n",
        "    total_zips = len(zip_files)\n",
        "    estimated_files_per_zip = len(list(Path(test_extract_dir).rglob('*.wav')))\n",
        "    total_estimated_files = total_zips * estimated_files_per_zip\n",
        "    \n",
        "    # æŽ¨å®šæ™‚é–“è¨ˆç®—\n",
        "    estimated_total_time = (total_estimated_files / files_per_sec) / 3600  # æ™‚é–“å˜ä½\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ“ˆ è¨ˆæ¸¬çµæžœ\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"å‡¦ç†é€Ÿåº¦: {files_per_sec:.2f} ãƒ•ã‚¡ã‚¤ãƒ«/ç§’\")\n",
        "    print(f\"æŽ¨å®šç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_estimated_files:,} ãƒ•ã‚¡ã‚¤ãƒ«\")\n",
        "    print(f\"æŽ¨å®šç·å‡¦ç†æ™‚é–“: {estimated_total_time:.2f} æ™‚é–“\")\n",
        "    print(f\"ZIPãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_zips}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "    shutil.rmtree(test_extract_dir)\n",
        "else:\n",
        "    print(\"âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "\n",
        "del compression_model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nâœ… é€Ÿåº¦è¨ˆæ¸¬å®Œäº†\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88cf0992",
      "metadata": {
        "id": "88cf0992"
      },
      "source": [
        "# ã‚»ãƒ« 3 â€” é †æ¬¡å‡¦ç†ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º (Sequential Tokenization)\n",
        "\n",
        "**æˆ¦ç•¥**: ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’1ã¤ãšã¤å±•é–‹ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ WAVå‰Šé™¤ ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚\n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€Colabã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’åœ§è¿«ã›ãšã«å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de228b0",
      "metadata": {
        "id": "0de228b0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "from audiocraft.models import CompressionModel\n",
        "import wandb\n",
        "\n",
        "# WandBåˆæœŸåŒ–\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "    if not wandb.api.api_key: wandb.login(key=WANDB_API_KEY)\n",
        "    wandb.init(project='musicgen-lora-finetune', name='sequential_tokenization', job_type='preprocessing')\n",
        "    use_wandb = True\n",
        "except: use_wandb = False\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
        "print('EnCodecãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...')\n",
        "compression_model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
        "compression_model.to('cuda')\n",
        "compression_model.eval()\n",
        "\n",
        "# å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
        "existing_tokens = set([Path(f).stem for f in glob.glob(os.path.join(TOKEN_DIR, '*.pt'))])\n",
        "print(f'æ—¢ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿: {len(existing_tokens)} ãƒ•ã‚¡ã‚¤ãƒ«')\n",
        "\n",
        "total_processed = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ãƒ«ãƒ¼ãƒ—\n",
        "for zip_idx, zip_file in enumerate(tqdm(zip_files, desc='ZIPãƒãƒƒãƒå‡¦ç†')):\n",
        "    print(f'\\nðŸ“¦ å‡¦ç†ä¸­: {zip_file.name} ({zip_idx+1}/{len(zip_files)})')\n",
        "\n",
        "    # 1. å±•é–‹ (Extract)\n",
        "    if os.path.exists(TEMP_DATA_DIR):\n",
        "        shutil.rmtree(TEMP_DATA_DIR)\n",
        "    os.makedirs(TEMP_DATA_DIR, exist_ok=True)\n",
        "\n",
        "    print('  â†³ å±•é–‹ä¸­...')\n",
        "    !tar -xzf \"{zip_file}\" -C \"{TEMP_DATA_DIR}\"\n",
        "\n",
        "    wav_files = list(Path(TEMP_DATA_DIR).rglob('*.wav'))\n",
        "    print(f'  â†³ {len(wav_files)} å€‹ã®WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º')\n",
        "\n",
        "    # 2. ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (Tokenize)\n",
        "    batch_processed = 0\n",
        "    for wav_path in tqdm(wav_files, desc='  â†³ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–', leave=False):\n",
        "        if wav_path.stem in existing_tokens:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(str(wav_path))\n",
        "            if sr != 32000:\n",
        "                resampler = torchaudio.transforms.Resample(sr, 32000)\n",
        "                wav = resampler(wav)\n",
        "            if wav.shape[0] > 1:\n",
        "                wav = wav.mean(dim=0, keepdim=True)\n",
        "            wav = wav.unsqueeze(0).to('cuda')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encoded_frames = compression_model.encode(wav)\n",
        "                tokens = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
        "\n",
        "            outp = os.path.join(TOKEN_DIR, wav_path.stem + '.pt')\n",
        "            torch.save({'tokens': tokens.cpu(), 'path': str(wav_path)}, outp)\n",
        "\n",
        "            batch_processed += 1\n",
        "            total_processed += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Error: {wav_path.name} - {e}')\n",
        "            continue\n",
        "\n",
        "    # 3. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (Cleanup)\n",
        "    print('  â†³ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...')\n",
        "    shutil.rmtree(TEMP_DATA_DIR)\n",
        "\n",
        "    # ãƒ­ã‚°\n",
        "    if use_wandb:\n",
        "        wandb.log({\n",
        "            'processed_files': total_processed,\n",
        "            'processed_zips': zip_idx + 1,\n",
        "            'files_in_batch': batch_processed\n",
        "        })\n",
        "\n",
        "print(f'\\nâœ… å…¨å‡¦ç†å®Œäº†: {total_processed} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã—ãŸ')\n",
        "if use_wandb: wandb.finish()\n",
        "\n",
        "del compression_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a3f7af",
      "metadata": {
        "id": "f7a3f7af"
      },
      "source": [
        "# ã‚»ãƒ« 4 â€” DeepSpeed è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b74acb",
      "metadata": {
        "id": "e2b74acb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "ds_config = {\n",
        "  \"train_batch_size\": 16,\n",
        "  \"train_micro_batch_size_per_gpu\": 2,\n",
        "  \"gradient_accumulation_steps\": 8,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": 1e-4,\n",
        "      \"weight_decay\": 0.01\n",
        "    }\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": True\n",
        "  },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 3,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"cpu\",\n",
        "      \"pin_memory\": True\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"cpu\"\n",
        "    },\n",
        "    \"overlap_comm\": True,\n",
        "    \"contiguous_gradients\": True\n",
        "  }\n",
        "}\n",
        "\n",
        "with open('ds_config.json','w') as f:\n",
        "    json.dump(ds_config, f, indent=2)\n",
        "\n",
        "print('ds_config.json ã‚’ä½œæˆã—ã¾ã—ãŸ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2967cb7b",
      "metadata": {
        "id": "2967cb7b"
      },
      "source": [
        "# ã‚»ãƒ« 5 â€” ãƒ¢ãƒ‡ãƒ«æº–å‚™ã¨LoRAé©ç”¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0395dd07",
      "metadata": {
        "id": "0395dd07"
      },
      "outputs": [],
      "source": [
        "import loralib as lora\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from audiocraft.models import MusicGen\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
        "print('MusicGen-Large ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...')\n",
        "model = MusicGen.get_pretrained('facebook/musicgen-large')\n",
        "model.eval()\n",
        "\n",
        "# LoRAé©ç”¨é–¢æ•°\n",
        "def apply_lora_to_linear(module, r=8, lora_alpha=32, lora_dropout=0.1, parent_name=''):\n",
        "    for name, child in list(module.named_children()):\n",
        "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
        "        if isinstance(child, nn.Linear):\n",
        "            lora_linear = lora.Linear(\n",
        "                child.in_features, child.out_features, r=r,\n",
        "                lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=(child.bias is not None)\n",
        "            )\n",
        "            lora_linear.weight.data = child.weight.data.clone()\n",
        "            if child.bias is not None:\n",
        "                lora_linear.bias.data = child.bias.data.clone()\n",
        "            setattr(module, name, lora_linear)\n",
        "            print(f'Applied LoRA to: {full_name}')\n",
        "        else:\n",
        "            apply_lora_to_linear(child, r, lora_alpha, lora_dropout, full_name)\n",
        "\n",
        "print('\\nLoRAã‚’é©ç”¨ä¸­...')\n",
        "apply_lora_to_linear(model.lm, r=8)\n",
        "\n",
        "# å‹¾é…è¨­å®š\n",
        "for p in model.parameters(): p.requires_grad = False\n",
        "for name, p in model.named_parameters():\n",
        "    if 'lora_' in name: p.requires_grad = True\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f'\\nå­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41971f7b",
      "metadata": {
        "id": "41971f7b"
      },
      "source": [
        "# ã‚»ãƒ« 6 â€” ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ä½œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba719a13",
      "metadata": {
        "id": "ba719a13"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "    def __init__(self, token_dir, max_length=1500):\n",
        "        self.files = sorted(list(Path(token_dir).glob('*.pt')))\n",
        "        self.max_length = max_length\n",
        "        print(f'Dataset initialized with {len(self.files)} samples')\n",
        "\n",
        "    def __len__(self): return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.load(self.files[idx])\n",
        "        tokens = data['tokens']\n",
        "        if tokens.dim() == 3: tokens = tokens.squeeze(0)\n",
        "\n",
        "        seq_len = tokens.shape[-1]\n",
        "        if seq_len > self.max_length:\n",
        "            start = random.randint(0, seq_len - self.max_length)\n",
        "            tokens = tokens[:, start:start + self.max_length]\n",
        "        elif seq_len < self.max_length:\n",
        "            pad = self.max_length - seq_len\n",
        "            tokens = torch.nn.functional.pad(tokens, (0, pad), value=0)\n",
        "        return tokens\n",
        "\n",
        "dataset = TokenDataset(TOKEN_DIR, max_length=1500)\n",
        "train_size = int(0.95 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f'Train: {len(train_dataset)}, Valid: {len(valid_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25dab26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå­¦ç¿’å®Œäº†å¾Œï¼‰\n",
        "print('ðŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...')\n",
        "final_checkpoint = {\n",
        "    'epoch': num_epochs - 1,\n",
        "    'global_step': global_step,\n",
        "    'model_state': model.state_dict(),\n",
        "    'opt_state': optimizer.state_dict(),\n",
        "    'loss': avg_loss,\n",
        "    'timestamp': time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "}\n",
        "final_lora_checkpoint = {\n",
        "    'epoch': num_epochs - 1,\n",
        "    'global_step': global_step,\n",
        "    'lora_weights': extract_lora_weights(model),\n",
        "    'loss': avg_loss,\n",
        "    'timestamp': final_checkpoint['timestamp']\n",
        "}\n",
        "\n",
        "torch.save(final_checkpoint, os.path.join(FULL_MODEL_DIR, 'final_model.pt'))\n",
        "torch.save(final_lora_checkpoint, os.path.join(LORA_DIR, 'final_lora.pt'))\n",
        "print('âœ“ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ')\n",
        "\n",
        "# WandBã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜\n",
        "if use_wandb:\n",
        "    try:\n",
        "        if os.path.exists(os.path.join(LORA_DIR, 'best_lora.pt')):\n",
        "            artifact = wandb.Artifact('best_lora_weights', type='model')\n",
        "            artifact.add_file(os.path.join(LORA_DIR, 'best_lora.pt'))\n",
        "            wandb.log_artifact(artifact)\n",
        "            print('âœ“ ãƒ™ã‚¹ãƒˆLoRAé‡ã¿ã‚’WandBã«ä¿å­˜ã—ã¾ã—ãŸ')\n",
        "        \n",
        "        artifact_final = wandb.Artifact('final_lora_weights', type='model')\n",
        "        artifact_final.add_file(os.path.join(LORA_DIR, 'final_lora.pt'))\n",
        "        wandb.log_artifact(artifact_final)\n",
        "        print('âœ“ æœ€çµ‚LoRAé‡ã¿ã‚’WandBã«ä¿å­˜ã—ã¾ã—ãŸ')\n",
        "        \n",
        "        wandb.finish()\n",
        "        print('âœ“ WandBã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã—ãŸ')\n",
        "    except Exception as e:\n",
        "        print(f'âš ï¸ WandBã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}')\n",
        "\n",
        "print('âœ… å…¨å‡¦ç†å®Œäº†')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da7a627b",
      "metadata": {
        "id": "da7a627b"
      },
      "source": [
        "# ã‚»ãƒ« 7 â€” å­¦ç¿’ãƒ«ãƒ¼ãƒ— (WandB + Checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47016277",
      "metadata": {
        "id": "47016277"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "\n",
        "# WandBåˆæœŸåŒ–\n",
        "try:\n",
        "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "    if not wandb.api.api_key: wandb.login(key=WANDB_API_KEY)\n",
        "    wandb.init(project='musicgen-lora-finetune', name='training_run', tags=['musicgen', 'lora', 'a100'])\n",
        "    use_wandb = True\n",
        "except: use_wandb = False\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "# ä¿å­˜å…ˆè¨­å®š\n",
        "CKPT_DIR = '/content/drive/MyDrive/MusicGen_Checkpoints'\n",
        "LORA_DIR = os.path.join(CKPT_DIR, 'lora_weights')\n",
        "FULL_MODEL_DIR = os.path.join(CKPT_DIR, 'full_models')\n",
        "os.makedirs(LORA_DIR, exist_ok=True)\n",
        "os.makedirs(FULL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "def extract_lora_weights(model):\n",
        "    return {name: param.cpu().detach().clone() for name, param in model.named_parameters() if 'lora_' in name and param.requires_grad}\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, global_step, loss, checkpoint_type='regular'):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch, 'global_step': global_step, 'model_state': model.state_dict(),\n",
        "        'opt_state': optimizer.state_dict(), 'loss': loss, 'best_loss': best_loss,\n",
        "        'timestamp': time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    }\n",
        "    lora_checkpoint = {\n",
        "        'epoch': epoch, 'global_step': global_step, 'lora_weights': extract_lora_weights(model),\n",
        "        'loss': loss, 'timestamp': checkpoint['timestamp']\n",
        "    }\n",
        "\n",
        "    if checkpoint_type == 'regular':\n",
        "        torch.save(checkpoint, os.path.join(CKPT_DIR, 'latest.pt'))\n",
        "        torch.save(lora_checkpoint, os.path.join(LORA_DIR, 'latest_lora.pt'))\n",
        "    elif checkpoint_type == 'best':\n",
        "        torch.save(checkpoint, os.path.join(FULL_MODEL_DIR, 'best_model.pt'))\n",
        "        torch.save(lora_checkpoint, os.path.join(LORA_DIR, 'best_lora.pt'))\n",
        "        print(f'  ðŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: Loss {loss:.4f}')\n",
        "    elif checkpoint_type == 'epoch':\n",
        "        torch.save(checkpoint, os.path.join(FULL_MODEL_DIR, f'checkpoint_epoch_{epoch+1}.pt'))\n",
        "        torch.save(lora_checkpoint, os.path.join(LORA_DIR, f'lora_epoch_{epoch+1}.pt'))\n",
        "    elif checkpoint_type == 'step':\n",
        "        torch.save(checkpoint, os.path.join(CKPT_DIR, f'checkpoint_step_{global_step}.pt'))\n",
        "\n",
        "# å†é–‹å‡¦ç†\n",
        "start_epoch = 0\n",
        "global_step = 0\n",
        "best_loss = float('inf')\n",
        "latest_ckpt = os.path.join(CKPT_DIR, 'latest.pt')\n",
        "if os.path.exists(latest_ckpt):\n",
        "    print(f'ðŸ”„ å†é–‹: {latest_ckpt}')\n",
        "    ckpt = torch.load(latest_ckpt, map_location='cpu')\n",
        "    model.load_state_dict(ckpt['model_state'], strict=False)\n",
        "    optimizer.load_state_dict(ckpt['opt_state'])\n",
        "    start_epoch = ckpt['epoch'] + 1\n",
        "    global_step = ckpt.get('global_step', 0)\n",
        "    best_loss = ckpt.get('best_loss', float('inf'))\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "num_epochs = 5\n",
        "accum_steps = 8\n",
        "\n",
        "print('\\nðŸš€ å­¦ç¿’é–‹å§‹')\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    for batch_idx, tokens in enumerate(pbar):\n",
        "        try:\n",
        "            tokens = tokens.to(device)\n",
        "            outputs = model.lm.forward(tokens)\n",
        "            loss = criterion(outputs, tokens.float()) / accum_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accum_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if use_wandb and global_step % 10 == 0:\n",
        "                    wandb.log({'train/loss': loss.item() * accum_steps, 'train/step': global_step})\n",
        "                if global_step % 500 == 0:\n",
        "                    save_checkpoint(model, optimizer, epoch, global_step, loss.item() * accum_steps, 'step')\n",
        "\n",
        "            epoch_loss += loss.item() * accum_steps\n",
        "            pbar.set_postfix({'loss': f'{loss.item() * accum_steps:.4f}'})\n",
        "        except Exception as e:\n",
        "            print(f'Error: {e}')\n",
        "            continue\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f'\\nEpoch {epoch+1} Loss: {avg_loss:.4f}')\n",
        "\n",
        "    if use_wandb: wandb.log({'epoch/loss': avg_loss, 'epoch': epoch+1})\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        save_checkpoint(model, optimizer, epoch, global_step, avg_loss, 'best')\n",
        "\n",
        "    save_checkpoint(model, optimizer, epoch, global_step, avg_loss, 'regular')\n",
        "    save_checkpoint(model, optimizer, epoch, global_step, avg_loss, 'epoch')\n",
        "\n",
        "print('ðŸŽ‰ å­¦ç¿’å®Œäº†')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0353a749",
      "metadata": {
        "id": "0353a749"
      },
      "source": [
        "# ã‚»ãƒ« 8 â€” æ¥½æ›²ç”Ÿæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e68fad",
      "metadata": {
        "id": "68e68fad"
      },
      "outputs": [],
      "source": [
        "from audiocraft.data.audio import audio_write\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "print('ðŸŽµ æ¥½æ›²ç”Ÿæˆ')\n",
        "model.eval()\n",
        "model.set_generation_params(use_sampling=True, top_k=250, duration=30)\n",
        "\n",
        "prompts = [\n",
        "    \"A dynamic heavy metal song with fast drums and guitar solo\",\n",
        "    \"Relaxing jazz piano with soft background ambience\",\n",
        "    \"Upbeat electronic dance music with strong bass\"\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    wav = model.generate(prompts)\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/MusicGen_Generated'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for idx, one_wav in enumerate(wav):\n",
        "    filename = os.path.join(output_dir, f\"generated_{idx}\")\n",
        "    audio_write(filename, one_wav.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "    print(f'\\nPrompt: {prompts[idx]}')\n",
        "    display(Audio(filename + \".wav\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
